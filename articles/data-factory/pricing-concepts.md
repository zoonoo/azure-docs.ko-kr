---
title: 예제를 통해 Azure Data Factory 가격 책정 이해
description: 이 문서에서는 자세한 예제와 함께 Azure Data Factory 가격 책정 모델을 설명하고 보여줍니다.
author: dcstwh
ms.author: weetok
ms.reviewer: jburchel
ms.service: data-factory
ms.topic: conceptual
ms.date: 09/14/2020
ms.openlocfilehash: 3bb9574c74aaa3c2589d0ca93fb906168ca99095
ms.sourcegitcommit: 32e0fedb80b5a5ed0d2336cea18c3ec3b5015ca1
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/30/2021
ms.locfileid: "104783373"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>예제를 통해 Data Factory 가격 책정 이해

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

이 문서에서는 자세한 예제와 함께 Azure Data Factory 가격 책정 모델을 설명하고 보여줍니다.

> [!NOTE]
> 아래 예제에 사용된 가격은 가상이며 실제 가격 책정을 의미하지는 않습니다.

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>AWS S3에서 Azure Blob Storage로 매시간 데이터 복사

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트가 있는 복사 작업

2. Azure Storage의 데이터에 대한 출력 데이터 세트

3. 파이프라인을 1시간마다 실행하는 일정 트리거

   ![다이어그램에서 일정 트리거를 사용하는 파이프라인을 보여 줍니다. 파이프라인에서 복사 작업은 A W S S3 연결된 서비스로 흐르는 입력 데이터 세트로 전달됩니다. 복사 작업은 또한 Azure Storage 연결된 서비스로 흐르는 출력 데이터 세트로 흐릅니다.](media/pricing-concepts/scenario1.png)

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 2개의 읽기/쓰기 엔터티  |
| 데이터 세트 생성 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 2개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 1개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 2개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 1개) |

**총 시나리오 가격 책정: $0.16811**

- Data Factory 작업 = **$0.0001**
  - 읽기/쓰기 = 10\*00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 2\*000005 = $0.00001 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.168**
  - 활동 실행 = 001\*2 = 0.002 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>데이터를 복사하고 Azure Databricks를 사용하여 시간별 변환

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하고 Azure Databricks를 사용하여 매시간 일정으로 데이터를 변환하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트 및 Azure Storage의 데이터에 대한 출력 데이터 세트가 있는 하나의 복사 작업
2. 데이터 변환에 대한 하나의 Azure Databricks 작업
3. 파이프라인을 1시간마다 실행하는 하나의 일정 트리거

![다이어그램에서 일정 트리거를 사용하는 파이프라인을 보여 줍니다. 파이프라인에서 복사 작업은 입력 데이터 세트, 출력 데이터 세트, 그리고 Azure Databricks에서 실행되는 DataBricks 작업으로 흐릅니다. 입력 데이터 세트는 A W S S3 연결된 서비스로 흐릅니다. 출력 데이터 세트는 Azure Storage 연결된 서비스로 흐릅니다.](media/pricing-concepts/scenario2.png)

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 3개의 읽기/쓰기 엔터티  |
| 데이터 세트 생성 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 3개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 2개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 3개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 2개) |
| Databricks 활동 실행 가정: 실행 시간 = 10분 | 10분 외부 파이프라인 활동 실행 |

**총 시나리오 가격 책정: $0.16916**

- Data Factory 작업 = **$0.00012**
  - 읽기/쓰기 = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 3\*000005 = $0.00001 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.16904**
  - 활동 실행 = 001\*3 = 0.003 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)
  - 외부 파이프라인 활동 = $0.000041(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.00025/시간)

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>데이터를 복사하고 동적 매개 변수를 사용하여 시간별 변환

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하고 Azure Databricks(스크립트의 동적 매개 변수와 함께)를 사용하여 매시간 일정으로 변환하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트 및 Azure Storage의 데이터에 대한 출력 데이터 세트가 있는 하나의 복사 작업
2. 변환 스크립트에 매개 변수를 동적으로 전달하는 하나의 조회 작업
3. 데이터 변환에 대한 하나의 Azure Databricks 작업
4. 파이프라인을 1시간마다 실행하는 하나의 일정 트리거

![다이어그램에서 일정 트리거를 사용하는 파이프라인을 보여 줍니다. 파이프라인에서 복사 작업은 입력 데이터 세트, 출력 데이터 세트, 그리고 Azure Databricks에서 실행되는 DataBricks 작업으로 흐르는 조회 활동으로 흐릅니다. 입력 데이터 세트는 A W S S3 연결된 서비스로 흐릅니다. 출력 데이터 세트는 Azure Storage 연결된 서비스로 흐릅니다.](media/pricing-concepts/scenario3.png)

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 3개의 읽기/쓰기 엔터티  |
| 데이터 세트 생성 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 4개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 3개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 4개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 3개) |
| 조회 작업 실행 가정: 실행 시간 = 1분 | 1분 파이프라인 활동 실행 |
| Databricks 활동 실행 가정: 실행 시간 = 10분 | 10분 외부 파이프라인 활동 실행 |

**총 시나리오 가격 책정: $0.17020**

- Data Factory 작업 = **$0.00013**
  - 읽기/쓰기 = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 4\*000005 = $0.00002 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.17007**
  - 활동 실행 = 001\*4 = 0.004 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)
  - 파이프라인 활동 = $0.00003(실행 시간의 1분에 대해 비례합니다. Azure Integration Runtime에서 $0.002/시간)
  - 외부 파이프라인 활동 = $0.000041(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.00025/시간)

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>일반 workday에 대한 매핑 데이터 흐름 디버그 사용

데이터 엔지니어인 Sam은 매일 매핑 데이터 흐름의 설계, 빌드 및 테스트를 담당합니다. Sam은 오전에 ADF UI에 로그인하고 데이터 흐름에 대해 디버그 모드를 사용하도록 설정합니다. 디버그 세션에 대한 기본 TTL은 60분입니다. Sam은 8시간 동안 하루 종일 작업하므로 디버그 세션이 만료되지 않습니다. 따라서 해당 날짜에 대한 Sam의 요금은 다음과 같습니다.

**8(시간) x 8(계산에 최적화된 코어) x $0.193 = $12.35**

동시에 다른 데이터 엔지니어인 Chris 또한 데이터 프로파일링 및 ETL 디자인 작업을 위해 ADF 브라우저 UI에 로그인합니다. Chris는 Sam과는 달리 ADF에서 하루 종일 작업을 하지는 않습니다. Chris는 Sam과 동일한 기간 및 동일 날짜에 1시간만 데이터 흐름 디버거를 사용하면 됩니다. 다음은 디버그 사용량과 관련하여 Chris에게 발생한 비용입니다.

**1(시간) x 8(범용 코어) x $0.274 = $2.19**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>매핑 데이터 흐름을 사용하여 BLOB 저장소의 데이터 변환

이 시나리오에서는 매시간 일정에 따라 ADF 매핑 데이터 흐름에서 시각적으로 Blob 저장소의 데이터를 변환하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. 변환 논리를 사용하는 데이터 흐름 활동입니다.

2. Azure Storage의 데이터에 대한 입력 데이터 세트

3. Azure Storage의 데이터에 대한 출력 데이터 세트

4. 파이프라인을 1시간마다 실행하는 일정 트리거

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 2개의 읽기/쓰기 엔터티  |
| 데이터 세트 생성 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 2개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 1개) |
| 데이터 흐름 가정: 실행 시간 = 10분 + 10분 TTL | \*TTL이 10인 일반컴퓨팅의 10 \* 16 코어 |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 2개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 1개) |

**총 시나리오 가격 책정: $1.4631**

- Data Factory 작업 = **$0.0001**
  - 읽기/쓰기 = 10\*00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 2\*000005 = $0.00001 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$1.463**
  - 활동 실행 = 001\*2 = 0.002 [1 run = $1/1000 = 0.001]
  - 데이터 흐름 활동 = $1.461, 20분(10분 실행 시간 + 10분 TTL)에 비례하여 계산됩니다. 16 코어 일반 컴퓨팅을 사용하는 Azure Integration Runtime의 $0.274/시간

## <a name="data-integration-in-azure-data-factory-managed-vnet"></a>Azure Data Factory 관리 VNET의 데이터 통합
이 시나리오에서는 Azure Blob Storage에서 원래 파일을 삭제하고, Azure SQL Database에서 Azure Blob Storage로 데이터를 복사하려고 합니다. 서로 다른 파이프라인에서 이 실행을 두 번 수행합니다. 이러한 두 파이프라인의 실행 시간은 겹칩니다.
![Scenario4](media/pricing-concepts/scenario-4.png) 시나리오를 달성하려면 다음 항목을 사용하여 2개의 파이프라인을 만들어야 합니다.
  - 파이프라인 활동 – 활동 삭제.
  - Azure Blob storage에서 복사될 데이터에 대한 입력 데이터 세트가 있는 복사 작업
  - Azure SQL Database의 데이터에 대한 출력 데이터 세트
  - 파이프라인을 실행하기 위한 일정 트리거입니다.


| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 4개의 읽기/쓰기 엔터티 |
| 데이터 세트 생성 | 8개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 4개, 연결된 서비스 참조에 대해 4개) |
| 파이프라인 만들기 | 6개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 2개, 데이터 세트 참조에 대해 4개) |
| 파이프라인 가져오기 | 2개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 6개의 활동 실행(트리거 실행에 대해 2개, 활동 실행에 대해 4개) |
| 활동 삭제 실행: 각 실행 시간은 5분입니다. 첫 번째 파이프라인에서의 활동 삭제 실행은 오전 10:00(UTC)부터 오전 10:05(UTC)까지입니다. 두 번째 파이프라인에서 활동 삭제 실행은 오전 10:02(UTC)부터 오전 10:07(UTC)까지입니다.|관리 VNET에서 총 7분 간 파이프라인 활동 실행. 파이프라인 활동은 관리 VNET에서 최대 50개의 동시 작업을 지원합니다. |
| 데이터 복사 가정: 각 실행 시간은 10 분입니다. 첫 번째 파이프라인의 복사 실행은 오전 10:06(UTC)부터 오전 10:15(UTC)까지입니다. 두 번째 파이프라인에서 활동 삭제 실행은 오전 10:08(UTC)부터 오전 10:17(UTC)까지입니다. | 10 * 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 2개의 실행만 발생했습니다. | 다시 시도되는 6개의 모니터링 실행 기록(파이프라인 실행에 대해 2개, 활동 실행에 대해 4개) |


**총 시나리오 가격 책정: $0.45523**

- Data Factory 작업 = $0.00023
  - 읽기/쓰기 = 20*00001 = $0.0002 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 6*000005 = $0.00003 [1 모니터링 = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 및 실행 = $0.455
  - 활동 실행 = 0.001*6 = 0.006 [1 실행 = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.333(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)
  - 파이프라인 활동 = $0.116(실행 시간의 7분에 대해 비례합니다. Azure Integration Runtime에서 $1/시간)

> [!NOTE]
> 이러한 가격은 설명을 돕기 위한 예시일 뿐입니다.

**FAQ**

Q: 50개 이상의 파이프라인 활동을 실행 하려는 경우, 이러한 활동을 동시에 실행할 수 있습니까?

A: 최대 50개의 동시 파이프라인 작업을 허용합니다.  51번 째 파이프라인 활동은 "빈 슬롯"이 열릴 때까지 큐에 대기합니다. 외부 활동에 대해서도 동일합니다. 최대 800개의 동시 외부 활동이 허용됩니다.

## <a name="next-steps"></a>다음 단계

Azure Data Factory에 대한 가격 책정을 이해했으므로 시작할 수 있습니다.

- [Azure Data Factory UI를 사용하여 데이터 팩터리 만들기](quickstart-create-data-factory-portal.md)

- [Azure Data Factory 소개](introduction.md)

- [Azure Data Factory에서 시각적 작성](author-visually.md)
