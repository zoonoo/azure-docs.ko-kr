---
title: 예제를 통한 Azure Data Factory 가격 이해
description: 이 문서에서는 자세한 예제와 함께 Azure Data Factory 가격 책정 모델을 설명하고 보여줍니다.
documentationcenter: ''
author: djpmsft
ms.author: daperlov
manager: jroth
ms.reviewer: maghan
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 09/14/2020
ms.openlocfilehash: a80e0f1b62257fdbce6598c9cc4088701cc2ae9c
ms.sourcegitcommit: bdd5c76457b0f0504f4f679a316b959dcfabf1ef
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 09/22/2020
ms.locfileid: "90983625"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>예제를 통해 Data Factory 가격 책정 이해

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

이 문서에서는 자세한 예제와 함께 Azure Data Factory 가격 책정 모델을 설명하고 보여줍니다.

> [!NOTE]
> 아래 예제에 사용된 가격은 가상이며 실제 가격 책정을 의미하지는 않습니다.

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>AWS S3에서 Azure Blob Storage로 매시간 데이터 복사

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트가 있는 복사 작업

2. Azure Storage의 데이터에 대한 출력 데이터 세트

3. 파이프라인을 1시간마다 실행하는 일정 트리거

   ![다이어그램에서 일정 트리거를 사용 하는 파이프라인을 보여 줍니다. 파이프라인에서 복사 작업은 입력 데이터 집합으로 전달 되 고,이는 W S S3 연결 된 서비스로 이동 하 고, 복사 작업은 Azure Storage 연결 된 서비스로 흐르는 출력 데이터 집합으로 흐릅니다.](media/pricing-concepts/scenario1.png)

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 2개의 읽기/쓰기 엔터티  |
| 데이터 세트 만들기 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 2개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 1개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 2개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 1개) |

**총 시나리오 가격 책정: $0.16811**

- Data Factory 작업 = **$0.0001**
  - 읽기/쓰기 = 10\*00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 2\*000005 = $0.00001 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.168**
  - 활동 실행 = 001\*2 = 0.002 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>데이터를 복사하고 Azure Databricks를 사용하여 시간별 변환

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하고 Azure Databricks를 사용하여 매시간 일정으로 데이터를 변환하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트 및 Azure Storage의 데이터에 대한 출력 데이터 세트가 있는 하나의 복사 작업
2. 데이터 변환에 대한 하나의 Azure Databricks 작업
3. 파이프라인을 1시간마다 실행하는 하나의 일정 트리거

![다이어그램에서 일정 트리거를 사용 하는 파이프라인을 보여 줍니다. 파이프라인에서 복사 작업은 입력 데이터 집합, 출력 데이터 집합 및 Azure Databricks에서 실행 되는 DataBricks 작업으로 흐릅니다. 입력 데이터 집합은 W S S3 연결 된 서비스로 흐릅니다. 출력 데이터 집합은 연결 된 Azure Storage 서비스로 흐릅니다.](media/pricing-concepts/scenario2.png)

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 3개의 읽기/쓰기 엔터티  |
| 데이터 세트 만들기 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 3개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 2개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 3개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 2개) |
| Databricks 활동 실행 가정: 실행 시간 = 10분 | 10분 외부 파이프라인 활동 실행 |

**총 시나리오 가격 책정: $0.16916**

- Data Factory 작업 = **$0.00012**
  - 읽기/쓰기 = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 3\*000005 = $0.00001 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.16904**
  - 활동 실행 = 001\*3 = 0.003 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)
  - 외부 파이프라인 활동 = $0.000041(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.00025/시간)

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>데이터를 복사하고 동적 매개 변수를 사용하여 시간별 변환

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하고 Azure Databricks(스크립트의 동적 매개 변수와 함께)를 사용하여 매시간 일정으로 변환하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트 및 Azure Storage의 데이터에 대한 출력 데이터 세트가 있는 하나의 복사 작업
2. 변환 스크립트에 매개 변수를 동적으로 전달하는 하나의 조회 작업
3. 데이터 변환에 대한 하나의 Azure Databricks 작업
4. 파이프라인을 1시간마다 실행하는 하나의 일정 트리거

![다이어그램에서 일정 트리거를 사용 하는 파이프라인을 보여 줍니다. 파이프라인에서 복사 작업은 입력 데이터 집합, 출력 데이터 집합 및 Azure Databricks에서 실행 되는 DataBricks 작업으로 이동 하는 조회 작업으로 흐릅니다. 입력 데이터 집합은 W S S3 연결 된 서비스로 흐릅니다. 출력 데이터 집합은 연결 된 Azure Storage 서비스로 흐릅니다.](media/pricing-concepts/scenario3.png)

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 3개의 읽기/쓰기 엔터티  |
| 데이터 세트 만들기 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 4개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 3개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 4개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 3개) |
| 조회 작업 실행 가정: 실행 시간 = 1분 | 1분 파이프라인 활동 실행 |
| Databricks 활동 실행 가정: 실행 시간 = 10분 | 10분 외부 파이프라인 활동 실행 |

**총 시나리오 가격 책정: $0.17020**

- Data Factory 작업 = **$0.00013**
  - 읽기/쓰기 = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 4\*000005 = $0.00002 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.17007**
  - 활동 실행 = 001\*4 = 0.004 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)
  - 파이프라인 활동 = $0.00003(실행 시간의 1분에 대해 비례합니다. Azure Integration Runtime에서 $0.002/시간)
  - 외부 파이프라인 활동 = $0.000041(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.00025/시간)

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>일반 workday에 대 한 매핑 데이터 흐름 디버그 사용

데이터 엔지니어는 Sam은 매일 데이터 흐름의 매핑 설계, 빌드 및 테스트를 담당 합니다. Sam은 오전에 ADF UI에 로그인 하 고 데이터 흐름에 대해 디버그 모드를 사용 하도록 설정 합니다. 디버그 세션에 대 한 기본 TTL은 60 분입니다. Sam은 8 시간 동안 하루 종일 작동 하므로 디버그 세션이 만료 되지 않습니다. 따라서 해당 날짜에 대 한 Sam 요금은 다음과 같습니다.

**8 (시간) x 8 (계산에 최적화 된 코어) x $0.193 = $12.35**

동시에 다른 데이터 엔지니어는 데이터 프로 파일링 및 ETL 디자인 작업을 위해 ADF 브라우저 UI에도 로그인 합니다. Chris는 Sam과 같은 매일 ADF에서 작동 하지 않습니다. Chris는 위의 Sam과 동일한 기간 동안 1 시간 동안 데이터 흐름 디버거를 사용 해야 합니다. 다음은 디버그 사용에 대 한 비용입니다.

**1 (시간) x 8 (범용 코어) x $0.274 = $2.19**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>매핑 데이터 흐름을 사용 하 여 blob 저장소의 데이터 변환

이 시나리오에서는 매시간 일정에 따라 ADF 매핑 데이터 흐름에서 시각적으로 Blob 저장소의 데이터를 변환 하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. 변환 논리를 사용 하는 데이터 흐름 활동입니다.

2. Azure Storage 데이터에 대 한 입력 데이터 집합입니다.

3. Azure Storage의 데이터에 대한 출력 데이터 세트

4. 파이프라인을 1시간마다 실행하는 일정 트리거

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 2개의 읽기/쓰기 엔터티  |
| 데이터 세트 만들기 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 2개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 1개) |
| 데이터 흐름 가정: 실행 시간 = 10 분 + 10 분 TTL | \*TTL이 10 인 일반 계산의 10 코어 코어 10 개 |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 2개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 1개) |

**총 시나리오 가격: $1.4631**

- Data Factory 작업 = **$0.0001**
  - 읽기/쓰기 = 10\*00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 2\*000005 = $0.00001 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$1.463**
  - 활동 실행 = 001\*2 = 0.002 [1 run = $1/1000 = 0.001]
  - 데이터 흐름 활동 = $1.461 20 분 (10 분 실행 시간 + 10 분 TTL)에 비례하여 계산 됩니다. 16 코어 일반 계산을 사용 하는 Azure Integration Runtime의 $0.274/시간

## <a name="data-integration-in-azure-data-factory-managed-vnet"></a>Azure Data Factory 관리 VNET의 데이터 통합
이 시나리오에서는 Azure Blob Storage에서 원래 파일을 삭제 하 고 Azure SQL Database에서 Azure Blob Storage으로 데이터를 복사 하려고 합니다. 다른 파이프라인에서이 실행을 두 번 수행 합니다. 이러한 두 파이프라인의 실행 시간은 겹칩니다.
![Scenario4 ](media/pricing-concepts/scenario-4.png) 시나리오를 수행 하려면 다음 항목을 포함 하는 두 개의 파이프라인을 만들어야 합니다.
  - 파이프라인 활동 – Delete 활동입니다.
  - Azure Blob storage에서 복사할 데이터의 입력 데이터 집합이 포함 된 복사 작업입니다.
  - Azure SQL Database 데이터에 대 한 출력 데이터 집합입니다.
  - 파이프라인을 실행 하기 위한 일정 트리거입니다.


| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 읽기/쓰기 엔터티 4 개 |
| 데이터 세트 만들기 | 8 개의 읽기/쓰기 엔터티 (데이터 집합을 만드는 경우 4, 연결 된 서비스 참조의 경우 4) |
| 파이프라인 만들기 | 읽기/쓰기 엔터티 6 개 (파이프라인 생성의 경우 2, 데이터 집합 참조의 경우 4 개) |
| 파이프라인 가져오기 | 2개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 작업 실행 6 개 (트리거 실행의 경우 2, 활동 실행의 경우 4 개) |
| Delete 작업 실행: 각 실행 시간은 5 분입니다. 첫 번째 파이프라인에서의 삭제 작업 실행은 오전 10:00 시부터 오전 10:05에서 UTC 사이입니다. 두 번째 파이프라인에서 삭제 작업 실행은 오전 10:02 시부터 오전 10:07에서 UTC 사이입니다.|관리 되는 VNET에서 총 7 분 파이프라인 활동 실행 파이프라인 활동은 관리 되는 VNET에서 최대 50의 동시성을 지원 합니다. |
| 데이터 복사 가정: 각 실행 시간은 10 분입니다. 첫 번째 파이프라인의 복사 실행은 오전 10:06 시부터 오전 10:15에서 UTC 사이입니다. 두 번째 파이프라인에서 삭제 작업 실행은 오전 10:08 시부터 오전 10:17에서 UTC 사이입니다. | 10 * 4 Azure Integration Runtime (기본 DIU 설정 = 4) 데이터 통합 단위에 대 한 자세한 내용과 복사 성능을 최적화 하는 방법에 대 한 자세한 내용은 [이 문서](copy-activity-performance.md) 를 참조 하세요. |
| 모니터 파이프라인 가정: 2 개 실행만 발생 했습니다. | 6 모니터링 실행 레코드 다시 시도 (파이프라인 실행의 경우 2 개, 활동 실행의 경우 4 개) |


**총 시나리오 가격: $0.45523**

- Data Factory 작업 = $0.00023
  - 읽기/쓰기 = 20 * 00001 = $0.0002 [1 R/W = $0.50/50000 = 0.00001]
  - Monitoring = 6 * 000005 = $0.00003 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 & 실행 = $0.455
  - 활동 실행 = 0.001 * 6 = 0.006 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.333 (실행 시간의 10 분에 비례하여 계산) Azure Integration Runtime에서 $0.25/시간)
  - 파이프라인 활동 = $0.116 (실행 시간 7 분 동안 비례) Azure Integration Runtime의 $1/시간)

> [!NOTE]
> 이러한 가격은 예를 들어 목적 으로만 사용 됩니다.

**FAQ**

Q: 50 개 이상의 파이프라인 활동을 실행 하려는 경우 이러한 활동을 동시에 실행할 수 있습니까?

A: 최대 50 동시 파이프라인 작업을 허용 합니다.  51th 파이프라인 활동은 "빈 슬롯"이 열릴 때까지 큐에 대기 됩니다. 외부 활동에 대해서도 동일 합니다. 최대 800 동시 외부 활동이 허용 됩니다.

## <a name="next-steps"></a>다음 단계

Azure Data Factory에 대한 가격 책정을 이해했으므로 시작할 수 있습니다.

- [Azure Data Factory UI를 사용 하 여 데이터 팩터리 만들기](quickstart-create-data-factory-portal.md)

- [Azure Data Factory 소개](introduction.md)

- [Azure Data Factory에서 시각적 작성](author-visually.md)
