---
title: '모델 평가: 모듈 참조'
titleSuffix: Azure Machine Learning service
description: Azure Machine Learning 서비스의 모델 평가 모듈을 사용 하 여 학습 된 모델의 정확도를 측정 하는 방법에 대해 알아봅니다.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: xiaoharper
ms.author: zhanxia
ms.date: 05/06/2019
ms.openlocfilehash: 17263c8e7300f427b7d82aea65e1f83edf6d6fc4
ms.sourcegitcommit: 07700392dd52071f31f0571ec847925e467d6795
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 08/28/2019
ms.locfileid: "70128859"
---
# <a name="evaluate-model-module"></a>모델 평가 모듈

이 문서에서는 Azure Machine Learning 서비스에 대 한 시각적 인터페이스 (미리 보기)의 모듈을 설명 합니다.

이 모듈을 사용 하 여 학습 된 모델의 정확도를 측정 합니다. 모델에서 생성 된 점수가 포함 된 데이터 집합을 제공 하 고 **모델 평가** 모듈은 산업 표준 평가 메트릭 집합을 계산 합니다.
  
 **평가 모델** 에서 반환 되는 메트릭은 평가 하는 모델 유형에 따라 달라 집니다.  
  
-   **분류 모델**    
-   **회귀 모델**    



> [!TIP]
> 모델 평가를 처음 접하는 경우 Dr으로 비디오 시리즈를 권장 합니다. Stephen Elston는 EdX에서 [기계 학습 과정](https://blogs.technet.microsoft.com/machinelearning/2015/09/08/new-edx-course-data-science-machine-learning-essentials/) 의 일부로 서 


**모델 평가** 모듈을 사용 하는 방법에는 세 가지가 있습니다.

+ 학습 데이터에 대 한 점수를 생성 하 고 이러한 점수에 따라 모델을 평가 합니다.
+ 모델에 점수를 생성 하지만 이러한 점수를 예약 된 테스트 집합의 점수와 비교 합니다.
+ 동일한 데이터 집합을 사용 하 여 서로 다른 두 개의 관련 모델에 대 한 점수를 비교 합니다.

## <a name="use-the-training-data"></a>학습 데이터 사용

모델을 평가 하려면 입력 열 및 점수 집합을 포함 하는 데이터 집합을 연결 해야 합니다.  다른 데이터를 사용할 수 없는 경우 원래 데이터 집합을 사용할 수 있습니다.

1. [점수 매기기 모델](./score-model.md) 의 **점수가 매겨진 데이터 집합** 출력을 **모델 평가**의 입력에 연결 합니다. 
2. **모델 평가** 모듈을 클릭 하 고 실험을 실행 하 여 평가 점수를 생성 합니다.

## <a name="use-testing-data"></a>테스트 데이터 사용

기계 학습에서 일반적인 시나리오는 [분할](./split-data.md) 모듈 또는 [파티션 및 샘플](./partition-and-sample.md) 모듈을 사용 하 여 원래 데이터 집합을 학습 및 테스트 데이터 집합으로 분리 하는 것입니다. 

1. [점수 매기기 모델](score-model.md) 의 **점수가 매겨진 데이터 집합** 출력을 **모델 평가**의 입력에 연결 합니다. 
2. 테스트 데이터를 포함 하는 분할 데이터 모듈의 출력을 **모델 평가**의 오른쪽 입력에 연결 합니다.
2. **모델 평가** 모듈을 클릭 하 고 **선택 된 실행** 을 선택 하 여 평가 점수를 생성 합니다.

## <a name="compare-scores-from-two-models"></a>두 모델의 점수 비교

두 번째 점수 집합을 연결 하 여 모델을 **평가할**수도 있습니다.  점수는 동일한 데이터에 대 한 다른 모델의 결과 집합 또는 알려진 결과가 있는 공유 평가 집합 일 수 있습니다.

이 기능은 동일한 데이터에서 서로 다른 두 모델의 결과를 쉽게 비교할 수 있기 때문에 유용 합니다. 또는 서로 다른 매개 변수를 사용 하 여 동일한 데이터에 대해 두 개의 다른 실행에서 점수를 비교할 수 있습니다.

1. [점수 매기기 모델](score-model.md) 의 **점수가 매겨진 데이터 집합** 출력을 **모델 평가**의 입력에 연결 합니다. 
2. 모델 점수 매기기 모듈의 출력을 **모델 평가**의 오른쪽 입력에 연결 합니다.
3. **모델 평가**를 마우스 오른쪽 단추로 클릭 하 고 **선택 된 실행** 을 선택 하 여 평가 점수를 생성 합니다.

## <a name="results"></a>결과

**모델 평가**를 실행 한 후 모듈을 마우스 오른쪽 단추로 클릭 하 고 **평가 결과** 를 선택 하 여 결과를 확인 합니다. 다음을 할 수 있습니다.

+ 다른 도구를 사용 하 여 쉽게 분석할 수 있도록 결과를 데이터 집합으로 저장
+ 인터페이스에서 시각화를 생성 합니다.

**모델 평가**의 두 입력에 데이터 집합을 연결 하는 경우 결과에는 두 데이터 집합 또는 두 모델 모두에 대 한 메트릭이 포함 됩니다.
왼쪽 포트에 연결 된 모델이 나 데이터가 먼저 보고서에 표시 된 다음 데이터 집합에 대 한 메트릭 또는 올바른 포트에 연결 된 모델이 표시 됩니다.  

예를 들어 다음 이미지는 동일한 데이터를 기반으로 하지만 다른 매개 변수를 사용 하는 두 클러스터링 모델의 결과 비교를 나타냅니다.  

![AML&#95;Comparing2Models](media/module/aml-comparing2models.png "AML_Comparing2Models")  

이는 클러스터링 모델 이므로 평가 결과는 두 회귀 모델의 점수를 비교 하거나 두 개의 분류 모델을 비교 하는 경우와 다릅니다. 그러나 전체 프레젠테이션은 동일 합니다. 

## <a name="metrics"></a>metrics

이 섹션에서는 **모델 평가**에서 사용 하도록 지원 되는 특정 유형의 모델에 대해 반환 되는 메트릭에 대해 설명 합니다.

+ [분류 모델](#bkmk_classification)
+ [회귀 모델](#bkmk_regression)

###  <a name="bkmk_classification"></a>분류 모델에 대 한 메트릭

분류 모델을 평가할 때 다음과 같은 메트릭이 보고 됩니다. 모델을 비교 하는 경우 평가를 위해 선택한 메트릭으로 순위가 매겨집니다.  
  
-   **정확도** 는 전체 사례에 대 한 실제 결과의 비율로 분류 모델의 적합도를 측정 합니다.  
  
-   **전체 자릿수** 는 모든 긍정 결과에 대 한 실제 결과의 비율입니다.  
  
-   **회수** 는 모델에서 반환 하는 모든 올바른 결과의 비율입니다.  
  
-   **F-점수** 는 전체 자릿수의 가중치가 적용 된 평균으로 계산 되며, 0과 1 사이의 값이 가장 좋습니다. 여기서 이상적인 F 점수 값은 1입니다.  
  
-   **Cc** 는 y 축에서 진정한 긍정을 사용 하 여 그린 곡선 아래의 면적을 측정 하 고 x 축에는 가양성을 측정 합니다. 이 메트릭은 여러 유형의 모델을 비교할 수 있는 단일 숫자를 제공 하기 때문에 유용 합니다.  
  
- **평균 로그 손실은** 잘못 된 결과에 대 한 패널티를 표현 하는 데 사용 되는 단일 점수입니다. 두 확률 분포 (true)와 모델에 있는 분포의 차이로 계산 됩니다.  
  
- **학습 로그 손실은** 임의 예측을 통해 분류자의 장점을 나타내는 단일 점수입니다. 로그 손실은 출력의 확률을 레이블의 알려진 값 (그라운드 참)과 비교 하 여 모델의 불확실성을 측정 합니다. 전체적으로 모델에 대 한 로그 손실을 최소화 하려고 합니다.

##  <a name="bkmk_regression"></a>회귀 모델에 대 한 메트릭
 
회귀 모델에 대해 반환 되는 메트릭은 일반적으로 오류 양을 예측 하도록 설계 되었습니다.  관찰 된 값과 예측 값의 차이가 적으면 모델은 데이터 웰에 맞게 고려 됩니다. 그러나 잔차의 패턴을 살펴보면 (한 예측 지점과 해당 하는 실제 값 간의 차이) 모델의 잠재적 바이어스에 대해 많은 정보를 확인할 수 있습니다.  
  
 회귀 모델 평가에 대해 다음과 같은 메트릭이 보고 됩니다. 모델을 비교할 때 평가를 위해 선택한 메트릭으로 순위가 매겨집니다.  
  
- **MAE (절대 평균 오차)** 는 실제 결과에 대 한 예측의 종료 방법을 측정 합니다. 따라서 점수가 낮을수록 좋습니다.  
  
- **RMSE (제곱 평균 제곱 오차)** 는 모델에서 오류를 요약 하는 단일 값을 만듭니다. 메트릭은 차이를 제곱 하 여 오버 예측과 예측에서의 차이를 무시 합니다.  
  
- **상대 절대 오차 (RAE)** 는 예상 값과 실제 값의 상대적 절대 차이입니다. 평균 차이는 산술 평균으로 나뉩니다.  
  
- **RSE (상대 제곱 오차** ) 마찬가지로 실제 값의 총 제곱 오차로 나누어 예측 값의 총 제곱 오차를 표준화 합니다.  
  
- **평균 0 개의 오류 (MZOE)** 는 예측이 올바른지 여부를 나타냅니다.  즉, `x!=y` `0`when 이면이 고, 그렇지 않으면입니다. `ZeroOneLoss(x,y) = 1`
  
- 일반적으로 R<sup>2</sup>라고도 하는 **결정 계수**는 모델의 예측 능력을 0에서 1 사이의 값으로 나타냅니다. 0은 모델이 무작위로 사용 됨을 의미 합니다 (아무 것도 설명 하지 않음). 1은 완벽 한 일치를 의미 합니다. 그러나 낮은 값은 완전히 정상이 고 높은 값은 주의 대상이 될 수 있으므로 R<sup>2</sup> 값을 해석 하는 데 주의를 기울여야 합니다.
  

## <a name="next-steps"></a>다음 단계

Azure Machine Learning 서비스에 [사용할 수 있는 모듈 집합](module-reference.md) 을 참조 하세요. 