---
title: Bing Speech 개념 | Microsoft Docs
titlesuffix: Azure Cognitive Services
description: Microsoft Speech Service에서 사용되는 기본 개념입니다.
services: cognitive-services
author: zhouwangzw
manager: wolfma
ms.service: cognitive-services
ms.subservice: bing-speech
ms.topic: article
ms.date: 09/18/2018
ms.author: zhouwang
ROBOTS: NOINDEX,NOFOLLOW
ms.openlocfilehash: 1cbf1514ac5eba4e288ecb78944878217fc5ba3e
ms.sourcegitcommit: d4dfbc34a1f03488e1b7bc5e711a11b72c717ada
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 06/13/2019
ms.locfileid: "65954525"
---
# <a name="basic-concepts"></a>기본 개념

[!INCLUDE [Deprecation note](../../../includes/cognitive-services-bing-speech-api-deprecation-note.md)]

이 페이지에서는 Microsoft 음성 인식 서비스에 대한 몇 가지 기본 개념에 대해 설명합니다. 애플리케이션에서 Microsoft 음성 인식 API를 사용하기 전에 먼저 이 페이지를 참조하는 것이 좋습니다.

## <a name="understanding-speech-recognition"></a>음성 인식 이해

음성 사용 애플리케이션을 처음 만들거나 기존 애플리케이션에 음성 기능을 처음 추가하는 경우 이 섹션을 사용하여 시작할 수 있습니다. 이미 음성 사용 애플리케이션을 사용한 경험이 있으면 이 섹션을 건너뛸 수 있습니다. 이러한 음성에 대한 경험이 많으면 이 섹션을 건너뛰고 프로토콜 세부 정보로 바로 이동할 수도 있습니다.

### <a name="audio-streams"></a>오디오 스트림

음성의 기본 개념 중 가장 중요한 것은 *오디오 스트림*입니다. 단일 시점에서 발생하고 단일 정보를 포함하는 키 입력과 달리 음성 요청은 수백 밀리초에 걸쳐 분산되며 수 킬로바이트의 정보를 포함합니다. 음성 발화의 기간은 애플리케이션에 간소화되고 세련된 음성 환경을 제공하려는 개발자에게 약간의 어려움을 안겨줍니다. 오늘날의 컴퓨터와 알고리즘에서는 발화 기간 중 절반 정도에서 음성 전사를 수행하므로 2초의 발화는 대략 1초 내에 전사될 수 있지만, 사용자를 처리하는 데 1초의 지연이 발생하는 애플리케이션은 간소화되지도 않고 세련되지도 않습니다.

다행히도 사용자가 다른 부분을 말하는 동안 발화의 한 부분에서 전사를 수행하여 전사 시간을 "숨기는" 방법이 있습니다. 예를 들어,는 1 초 utterance 100 밀리초의 10 개의 청크로 분할 하 여 각 청크에서 기록을 차례로 수행 하 여를 기록 하는 데 필요한 총 500 밀리초의 수 "숨김" 450 개는 사용자를 인식 되지 않도록 기록 이며 말하기는 해당 하는 동안 수행 중입니다. 이 예를 생각해 보면, 사용자가 다음 100밀리초 동안 말하는 동안 서비스에서 이전의 100밀리초에 대한 오디오에서 전사를 수행하므로, 사용자가 말을 멈추는 경우 서비스에서 대략 100밀리초의 오디오를 전사하여 결과를 생성하면 됩니다.

이러한 사용자 경험을 달성하기 위해, 음성 오디오 정보는 청크로 수집되어 사용자가 말하는 대로 전사됩니다. 이러한 오디오 청크는 *오디오 스트림*에서 집합적으로 수집되며, 이러한 오디오 청크를 서비스로 보내는 프로세스를 *오디오 스트리밍*이라고 합니다. 오디오 스트리밍은 음성 사용 애플리케이션의 중요한 부분이며, 청크 크기 조정 및 스트리밍 구현 최적화는 애플리케이션의 사용자 경험을 향상시키는 데 가장 효과적인 방법에 해당합니다.

### <a name="microphones"></a>마이크

사람은 귀를 통해 음성 오디오를 처리하지만, 무생물 하드웨어는 마이크를 사용합니다. 모든 애플리케이션에서 음성을 사용할 수 있게 하려면 애플리케이션에 오디오 스트림을 제공하는 마이크와 통합해야 합니다.

마이크에 대한 API는 마이크에서 오디오 바이트 수신을 시작하고 중지할 수 있어야 합니다. 마이크를 트리거하는 사용자 작업을 결정하여 음성 수신을 시작해야 합니다. 단추를 눌러 수신 시작을 트리거하도록 선택할 수 있거나, *key word*(핵심 단어) 또는 *wake word*(각성 단어) 감시기(spotter)에서 항상 마이크를 청취하도록 선택하고 해당 모듈의 출력을 사용하여 오디오를 Microsoft Speech Service로 보내도록 트리거할 수 있습니다.

### <a name="end-of-speech"></a>음성 끝

화자가 말을 *멈춘* *시점*을 감지하는 것은 사람에게는 충분히 간단해 보이지만 랩 환경 외부에서는 다소 어려운 문제입니다. 주변 소음이 많아 상황을 복잡하게 만드는 경우가 종종 있으므로 발화 후에 순수한 묵음을 찾는 것만으로는 충분하지 않습니다. Microsoft Speech Service는 사용자가 말을 멈추었을 때 빠르게 감지하는 작업을 훌륭하게 수행하고 이 사실을 애플리케이션에 알릴 수 있지만, 이 배열은 애플리케이션에서 사용자가 말을 멈추는 시점을 마지막으로 알고 있음을 의미합니다. 이는 애플리케이션에서 사용자 입력이 *시작되고 끝나는 시점*을 이 *가장 먼저* 알 수 있는 다른 입력 형식과는 전혀 다릅니다.

### <a name="asynchronous-service-responses"></a>비동기 서비스 응답

사용자 입력이 완료되었을 때 애플리케이션에 알려야 한다는 사실은 애플리케이션에 성능 저하 또는 프로그래밍 문제를 초래하지는 않지만, 친숙한 입력 요청/응답 패턴과는 다른 방식으로 음성 요청을 고려해야 합니다. 애플리케이션에서 사용자가 말하기를 멈추는 시점을 알 수 없으므로 애플리케이션은 서비스의 응답을 동시에 비동기적으로 기다리면서 오디오를 서비스로 계속 스트림해야 합니다. 이 패턴은 HTTP와 같은 다른 요청/응답 웹 프로토콜과 다릅니다. 이러한 프로토콜에서는 응답을 받기 전에 요청을 완료해야 하며, Microsoft Speech Service 프로토콜에서는 *요청에 대한 오디오를 계속 스트림하면서* 응답을 받습니다.

> [!NOTE]
> Speech HTTP REST API를 사용하는 경우 이 기능은 지원되지 않습니다.

### <a name="turns"></a>턴

말은 정보의 이동 수단입니다. 말하는 경우 해당 정보를 듣고 있는 누군가에게 소유하고 있는 정보를 전달하려고 합니다. 정보를 전달할 때는 일반적으로 말하기와 듣기가 교대로 수행됩니다. 마찬가지로, 음성 사용 애플리케이션은 일반적으로 수신과 응답을 교대로 수행하면서 사용자와 상호 작용하지만 대개 애플리케이션이 대부분의 듣기를 수행합니다. 사용자의 음성 입력 및 이 입력에 대한 서비스 응답을 *턴*이라고 합니다. *턴*은 사용자가 말하면 시작되고, 애플리케이션에서 음성 서비스 응답 처리를 완료하면 종료됩니다.

### <a name="telemetry"></a>원격 분석

숙련된 개발자인 경우에도 음성 사용 장치 또는 애플리케이션을 만드는 것은 어려울 수 있습니다. 스트림 기반 프로토콜은 얼핏 보기에는 어려운 것처럼 보이지만 묵음 감지와 같은 중요한 세부 정보는 전혀 새로운 것일 수 있습니다. 단일 요청/응답 쌍을 완료하기 위해 많은 메시지를 성공적으로 보내고 받아야 하기 때문에 메시지에 대한 완전하고 정확한 데이터를 수집하는 것이 *매우* 중요합니다. Microsoft Speech Service 프로토콜은 이러한 데이터의 수집을 제공합니다. 가능한 한 정확하게 필요한 데이터를 제공하기 위해 모든 노력을 기울여야 합니다. 완전하고 정확한 데이터를 제공함으로써 자신에게 도움이 됩니다. 클라이언트 구현 문제를 해결하는 데 Microsoft Speech Service 팀의 도움이 필요하면 수집한 원격 분석 데이터의 품질이 문제 분석에 매우 중요합니다.

> [!NOTE]
> 음성 인식 REST API를 사용하는 경우 이 기능은 지원되지 않습니다.

### <a name="speech-application-states"></a>음성 애플리케이션 상태

애플리케이션에서 음성 입력을 사용할 수 있도록 하는 단계는 마우스 클릭이나 손가락 탭과 같은 다른 입력 형태의 단계와 약간 다릅니다. 애플리케이션에서 마이크를 청취하고 음성 서비스에 데이터를 보내는 시점, 서비스에서 응답을 기다리는 시점 및 유휴 상태에 있는 시점을 추적해야 합니다. 이러한 상태 간의 관계는 아래 다이어그램에 나와 있습니다.

![음성 애플리케이션 상태 다이어그램](Images/speech-application-state-diagram.png)

Microsoft Speech Service는 일부 상태에 참여하므로 서비스 프로토콜에는 상태 간 애플리케이션 전환에 도움이 되는 메시지가 정의됩니다. 애플리케이션은 이러한 프로토콜 메시지를 해석하고 작동하여 음성 애플리케이션 상태를 추적하고 관리해야 합니다.

## <a name="using-the-speech-recognition-service-from-your-apps"></a>앱에서 음성 인식 서비스 사용

개발자가 앱에 Speech를 추가할 수 있도록 Microsoft 음성 인식 서비스에서 제공하는 두 가지 방법은 다음과 같습니다.

- [REST API](GetStarted/GetStartedREST.md): 개발자는 음성 인식을 위해 앱에서 서비스로의 HTTP 호출을 사용할 수 있습니다.
- [클라이언트 라이브러리](GetStarted/GetStartedClientLibraries.md): 개발자는 고급 기능용으로 Microsoft Speech 클라이언트 라이브러리를 다운로드하여 앱에 연결할 수 있습니다.  클라이언트 라이브러리는 다양한 언어(C#, Java, JavaScript, ObjectiveC)를 사용하여 다양한 플랫폼(Windows, Android, iOS)에서 사용할 수 있습니다.

| 사용 사례 | [REST API](GetStarted/GetStartedREST.md) | [클라이언트 라이브러리](GetStarted/GetStartedClientLibraries.md) |
|-----|-----|-----|
| 중간 결과가 없는 짧은 음성 오디오(예: 오디오 길이가 15초 미만인 명령) 변환 | 예 | 예 |
| 긴 오디오(15초 초과) 변환 | 아닙니다. | 예 |
| 중간 결과가 필요한 오디오 스트림 | 아닙니다. | 예 |
| LUIS를 사용하여 오디오에서 변환된 텍스트 인식 | 아닙니다. | 예 |

 언어 또는 플랫폼에 아직 SDK가 없는 경우 [프로토콜 설명서](API-Reference-REST/websocketprotocol.md)에 따라 사용자 고유의 구현을 만들 수 있습니다.

## <a name="recognition-modes"></a>인식 모드

인식에는 `interactive`, `conversation` 및 `dictation`의 세 가지 인식 모드가 있습니다. 인식 모드는 사용자가 말하는 방법에 따라 음성 인식을 조정합니다. 애플리케이션에 적합한 인식 모드를 선택합니다.

> [!NOTE]
> REST 프로토콜의 인식 모드는 WebSocket 프로토콜의 인식 모드와 다른 동작을 수행할 수 있습니다. 예를 들어 REST API는 대화 또는 받아쓰기 모드에서도 연속 인식을 지원하지 않습니다.
> [!NOTE]
> 이러한 모드는 REST 또는 WebSocket 프로토콜을 직접 사용할 때 적용됩니다. [클라이언트 라이브러리](GetStarted/GetStartedClientLibraries.md)는 다른 매개 변수를 사용하여 인식 모드를 지정합니다. 자세한 내용은 선택한 클라이언트 라이브러리를 참조하세요.

Microsoft Speech Service는 모든 인식 모드에 대해 하나의 인식 구 결과만 반환합니다. 한 번의 발화에는 15초의 제한이 있습니다.

### <a name="interactive-mode"></a>대화형 모드

`interactive` 모드에서는 사용자가 짧은 요청을 하고 애플리케이션에서 응답으로 작업을 수행해야 합니다.

대화형 모드 애플리케이션의 일반적인 특징은 다음과 같습니다.

- 사용자는 다른 사람에게 말하는 것이 아니라 컴퓨터에 말한다는 것을 알고 있습니다.
- 애플리케이션 사용자는 애플리케이션에서 수행하려는 작업에 따라 말하려는 내용을 미리 알고 있습니다.
- 발화는 일반적으로 2-3초 정도 지속됩니다.

### <a name="conversation-mode"></a>대화 모드

`conversation` 모드에서는 사용자가 사람 사이의 대화에 참여합니다.

대화 모드 애플리케이션의 일반적인 특징은 다음과 같습니다.

- 사용자는 다른 사람에게 말한다는 것을 알고 있습니다.
- 음성 인식은 두 참가자 중 한 사람 또는 두 사람 모두가 음성 텍스트를 볼 수 있게 하여 인간의 대화를 향상시킵니다.
- 사용자는 자신이 말하려는 내용을 항상 계획하지는 않습니다.
- 사용자는 속어와 기타 정중하지 않은 음성을 자주 사용합니다.

### <a name="dictation-mode"></a>받아쓰기 모드

`dictation` 모드에서는 사용자가 추가 처리를 위해 더 긴 발화를 애플리케이션에 말합니다.

받아쓰기 모드 애플리케이션의 일반적인 특징은 다음과 같습니다.

- 사용자는 컴퓨터에 말한다는 것을 알고 있습니다.
- 음성 인식 텍스트 결과는 사용자에게 표시됩니다.
- 사용자는 종종 말하려는 내용을 계획하고 더 정중한 언어를 사용합니다.
- 사용자는 5~8초 동안 지속되는 완전한 문장을 사용합니다.

> [!NOTE]
> 받아쓰기 및 대화 모드에서 Microsoft Speech Service는 부분 결과를 반환하지 않습니다. 대신, 오디오 스트림의 묵음 경계 이후의 안정적인 구 결과를 반환합니다. Microsoft는 이러한 연속 인식 모드에서 사용자 경험을 향상시키기 위해 음성 프로토콜을 향상시킬 수 있습니다.

## <a name="recognition-languages"></a>인식 언어

*인식 언어*는 애플리케이션 사용자가 말하는 언어를 지정합니다. 연결에서 *language* URL 쿼리 매개 변수를 사용하여 *인식 언어*를 지정합니다. *language* 쿼리 매개 변수의 값은 [BCP 47](https://en.wikipedia.org/wiki/IETF_language_tag) IETF 언어 태그를 사용하고, 음성 인식 API에서 지원되는 언어 중 **하나**여야 합니다. Speech Service에서 지원되는 언어의 전체 목록은 [지원되는 언어](API-Reference-REST/supportedlanguages.md) 페이지에서 찾을 수 있습니다.

Microsoft Speech Service는 `HTTP 400 Bad Request` 응답을 표시하여 잘못된 연결 요청을 거부합니다. 잘못된 요청은 다음 중 하나입니다.

- *language* 쿼리 매개 변수 값을 포함하고 있지 않습니다.
- 형식이 잘못 지정된 *language* 쿼리 매개 변수를 포함하고 있습니다.
- 지원되는 언어 중 하나가 아닌 *language* 쿼리 매개 변수를 포함하고 있습니다.

서비스에서 지원되는 언어 중 하나 또는 모두를 지원하는 애플리케이션을 빌드하도록 선택할 수 있습니다.

### <a name="example"></a>예

다음 예제에서는 애플리케이션에서 미국 영어 화자에 대해 *대화* 음성 인식 모드를 사용합니다.

```HTTP
https://speech.platform.bing.com/speech/recognition/conversation/cognitiveservices/v1?language=en-US
```

## <a name="transcription-responses"></a>전사 응답

필사 응답은 변환된 텍스트를 오디오에서 클라이언트로 반환합니다. 전사 응답에 포함되는 필드는 다음과 같습니다.

- `RecognitionStatus`는 인식 상태를 지정합니다. 아래 표에는 가능한 값이 나와 있습니다.

| 상태 | 설명 |
| ------------- | ---------------- |
| 성공 | 성공적으로 인식했고 DisplayText 필드가 있습니다. |
| NoMatch | 오디오 스트림에서 음성이 감지되었지만 대상 언어의 단어가 일치하지 않습니다. 자세한 내용은 [NoMatch 인식 상태(#nomatch-recognition-status)]를 참조하세요.  |
| InitialSilenceTimeout | 오디오 스트림의 시작 부분에는 묵음만 있으며, 서비스의 음성 대기 시간이 초과되었습니다. |
| BabbleTimeout | 오디오 스트림의 시작 부분에는 소음만 있으며, 서비스의 음성 대기 시간이 초과되었습니다. |
| 오류 | 인식 서비스에서 내부 오류가 발생하여 계속할 수 없습니다. |

- `DisplayText`는 대문자 표시, 문장 부호 및 역 텍스트 정규화가 적용되고 불경한 언어가 별표로 마스킹된 후에 인식된 구를 나타냅니다. `RecognitionStatus` 필드의 값이 `Success`인 *경우에만* DisplayText 필드가 있습니다.

- `Offset`은 오디오 스트림의 시작을 기준으로 구가 인식된 오프셋(100나노초 단위)을 지정합니다.

- `Duration`은 이 음성 구의 지속 시간(100나노초 단위)을 지정합니다.

필요한 경우 전사 응답에서 더 많은 정보를 반환합니다. 더 자세한 출력을 반환하는 방법은 [출력 형식](#output-format)을 참조하세요.

Microsoft Speech Service는 대문자 표시/문장 부호 추가, 불경한 언어 마스킹 및 일반 형식으로의 텍스트 정규화가 포함된 추가 전사 프로세스를 지원합니다. 예를 들어 사용자가 "여섯 개의 아이폰을 구입하라고 미리 알려주세요"라는 단어로 표현된 구를 말하면 Microsoft Speech Service에서 "6개의 아이폰을 구입하라고 미리 알려주세요"라고 전사된 텍스트를 반환합니다. "여섯"이라는 단어를 "6"이라는 숫자로 변환하는 프로세스를 *ITN*(*역 텍스트 정규화*)라고 합니다.

### <a name="nomatch-recognition-status"></a>NoMatch 인식 상태

Microsoft Speech Service가 오디오 스트림에서 음성을 감지하지만 해당 음성을 요청에 사용되는 언어 문법과 일치시킬 수 없는 경우 전사 응답에서 `RecognitionStatus`에 `NoMatch`를 반환합니다. 예를 들어 인식기에서 미국 영어를 음성 언어로 사용해야 하는 경우 사용자가 독일어로 무언가를 말하면 *NoMatch* 상태가 발생할 수 있습니다. 발화의 파형 패턴은 사람의 음성이 있음을 나타내지만, 발화된 단어 중 어느 것도 인식기에서 사용하는 미국 영어 어휘집과 일치하지 않습니다.

또 다른 *NoMatch* 상태는 인식 알고리즘에서 오디오 스트림에 포함된 소리에 대한 정확한 일치를 찾을 수 없는 경우에 발생합니다. 이 상태가 발생하면 Microsoft Speech Service에서 *가설 텍스트*가 포함된 *speech.hypothesis* 메시지를 생성할 수 있지만 *RecognitionStatus*가 *NoMatch*인 *speech.phrase* 메시지를 생성합니다. 이 상태는 정상입니다. *speech.hypothesis* 메시지에서 텍스트의 정확도 또는 충실도에 대해 어떤 가정도 하면 안됩니다. 또한 Microsoft Speech Service에서 *RecognitionStatus* *Success*가 있는 *speech.phrase* 메시지를 생성할 수 있는 *speech.hypothesis* 메시지를 생성하므로 이를 가정하면 안됩니다.

## <a name="output-format"></a>출력 형식

Microsoft Speech Service는 전사 응답에 다양한 페이로드 형식을 반환할 수 있습니다. 모든 페이로드는 JSON 구조입니다.

`format` URL 쿼리 매개 변수를 지정하여 구 결과 형식을 제어할 수 있습니다. 기본적으로 서비스는 `simple` 결과를 반환합니다.

| 형식 | 설명 |
|-----|-----|
| `simple` | 인식 상태와 인식된 텍스트가 표시 형식으로 포함된 간소화된 구 결과입니다. |
| `detailed` | 각 구 결과에 네 가지 인식 형식과 신뢰도 점수가 모두 포함된 구 검색 결과의 인식 상태 및 N 상위 목록입니다. |

`detailed` 형식에는 응답에서 `RecognitionStatus`, `Offset` 및 `duration`외에도 [N 상위 값](#n-best-values)이 포함됩니다.

### <a name="n-best-values"></a>N 상위 값

수신기는 인간이든 컴퓨터이든 간에 이들이 말한 것을 *정확히* 들었는지 확신할 수 없습니다. 수신기는 발화의 특정 해석에만 *확률*을 할당할 수 있습니다.

정상 조건에서 자주 상호 작용하는 다른 사람에게 말할 때 사람들은 발화된 단어를 인식할 확률이 높습니다. 컴퓨터 기반 음성 수신기는 비슷한 정확도 수준을 달성하기 위해 노력하고, 적절한 조건에서 [인간과의 동등성을 달성합니다](https://blogs.microsoft.com/next/2016/10/18/historic-achievement-microsoft-researchers-reach-human-parity-conversational-speech-recognition/#sm.001ykosqs14zte8qyxj2k9o28oz5v).

음성 인식에 사용되는 알고리즘은 정상 처리의 일환으로 발화에 대한 대체 해석을 탐색합니다. 일반적으로 이러한 대체 해석은 단일 해석을 강력하게 지지하는 증거가 되면 무시됩니다. 그러나 최적 조건에 미치지 못하면 음성 인식기에서 가능한 다른 해석 목록으로 마무리됩니다. 이 목록의 상위 *N*개 대체 해석을 *N 상위 목록*이라고 합니다. 각 대체 해석에는 [신뢰도 점수](#confidence)가 할당됩니다. 신뢰도 점수의 범위는 0에서 1까지입니다. 1점은 가장 높은 신뢰도를 나타내며, 0점은 가장 낮은 신뢰도를 나타냅니다.

> [!NOTE]
> N 상위 목록의 항목 수는 여러 발화에 따라 달라집니다. 항목 수는 *동일한* 발화의 여러 인식에 따라 달라질 수 있습니다. 이 변형은 음성 인식 알고리즘의 확률적 특성에 따른 당연하고 예상된 결과입니다.

N 상위 목록에 반환되는 각 항목은 다음과 같습니다.

- `Confidence` - 이 항목의 [신뢰도 점수](#confidence)를 나타냅니다.
- `Lexical` - 인식된 텍스트의 [어휘 형식](#lexical-form)입니다.
- `ITN` - 인식된 텍스트의 [ITN 형식](#itn-form)입니다.
- `MaskedITN` - 인식된 텍스트의 [마스킹된 ITN 형식](#masked-itn-form)입니다.
- `Display` - 인식된 텍스트의 [표시 형식](#display-form)입니다.

### 신뢰도 점수 <a id="confidence"></a>

신뢰도 점수는 음성 인식 시스템에 필수적인 요소입니다. Microsoft Speech Service는 *신뢰도 분류자*로부터 신뢰도 점수를 얻습니다. Microsoft는 정확한 인식과 잘못된 인식을 최대한 구별할 수 있도록 설계된 기능 집합을 통해 신뢰도 분류자를 학습합니다. 신뢰도 점수는 개별 단어와 전체 발화에 대해 평가됩니다.

서비스에서 반환된 신뢰도 점수를 사용하도록 선택하는 경우 다음 동작을 알고 있어야 합니다.

- 신뢰도 점수는 동일한 인식 모드와 언어 내에서만 비교할 수 있습니다. 다른 언어 또는 다른 인식 모드 간에 점수를 비교하지 않습니다. 예를 들어 대화형 인식 모드의 신뢰도 점수는 받아쓰기 모드의 신뢰도 점수와 *상관 관계가 없습니다*.
- 신뢰도 점수는 제한된 발화 집합에 가장 적합합니다. 큰 발화 집합에 대한 점수에는 당연히 상당한 정도의 가변성이 있습니다.

신뢰도 점수 값을 애플리케이션이 작동하는 *임계값*으로 사용하도록 선택하는 경우 음성 인식을 사용하여 임계값을 설정합니다.

- 애플리케이션에 대한 발화 대표 샘플에서 음성 인식을 실행합니다.
- 샘플 집합의 각 인식에 대한 신뢰도 점수를 수집합니다.
- 해당 샘플에 대한 신뢰도의 일부 백분위수를 기준으로 임계값을 설정합니다.

모든 애플리케이션에 대해 단일 임계값을 설정하는 것은 적절하지 않습니다. 한 애플리케이션에 허용 가능한 신뢰도 점수가 다른 애플리케이션에는 허용되지 않을 수도 있습니다.

### <a name="lexical-form"></a>어휘 형식

어휘 형식은 문장 부호 또는 대문자 표시가 없이 정확히 어떻게 발생했는지를 나타내는 인식된 텍스트입니다. 예를 들어 "1020 엔터프라이즈 가도" 주소의 어휘 형식은 "일십 이십 엔터프라이즈 가도"로 말한 것으로 가정하면 *일십 이십 엔터프라이즈 가도*가 됩니다. "5개의 아이폰을 구입하라고 미리 알려주세요"라는 문장의 어휘 형식은 *다섯 개의 아이폰을 구입하라고 미리 알려주세요*입니다.

어휘 형식은 비표준 텍스트 정규화를 수행해야 하는 애플리케이션에 가장 적합합니다. 어휘 형식은 처리되지 않은 인식 단어가 필요한 애플리케이션에도 적합합니다.

불경한 언어는 어휘 형식으로 마스킹되지 않습니다.

### <a name="itn-form"></a>ITN(역 텍스트 정규화) 형식

텍스트 정규화는 한 형식에서 다른 형식의 "정규" 형식으로 텍스트를 변환하는 프로세스입니다. 예를 들어 "555-1212" 전화 번호는 *오 오 오 일 이 일 이* 정규 형식으로 변환할 수 있습니다. *역* 텍스트 정규화(ITN)는 이 프로세스를 역방향으로 수행하여 "오 오 오 일 이 일 이"라는 단어를 반전된 *555-1212* 정규 형식으로 변환합니다. 인식 결과의 ITN 형식에는 대문자 표시 또는 문장 부호가 포함되지 않습니다.

ITN 형식은 인식된 텍스트에서 작동하는 애플리케이션에 가장 적합합니다. 예를 들어 사용자가 검색어를 말한 다음, 웹 쿼리에서 이러한 용어를 사용할 수 있도록 허용하는 애플리케이션은 ITN 형식을 사용합니다. 불경한 언어는 ITN 형식으로 마스킹되지 않습니다. 불경한 언어를 마스킹하려면 *마스킹된 ITN 형식*을 사용합니다.

### <a name="masked-itn-form"></a>마스킹된 ITN 형식

불경한 언어는 당연히 음성 언어의 일부이기 때문에 이러한 단어와 구가 발화되면 Microsoft Speech Service에서 이를 인식합니다. 그러나 불경한 언어는 모든 애플리케이션, 특히 제한된 비성인 사용자 대상 그룹이 있는 애플리케이션에 적합하지 않을 수 있습니다.

마스킹된 ITN 형식은 불경한 언어 마스킹을 역 텍스트 정규화 형식에 적용합니다. 불경한 언어를 마스킹하려면 불경한 언어 매개 변수 값을 `masked`로 설정합니다. 불경한 언어를 마스킹하는 경우 언어의 불경한 어휘집에 속한 것으로 인식되는 단어는 별표로 바뀝니다. 예를 들어 *5 ****개의 연필을 구입하라고 미리 알려주세요*와 같습니다. 인식 결과의 마스킹된 ITN 형식에는 대문자 표시 또는 문장 부호가 포함되지 않습니다.

> [!NOTE]
> 불경한 언어 쿼리 매개 변수 값이 `raw`로 설정되면 마스킹된 ITN 형식이 ITN 형식과 같습니다. 이에 따라 불경한 언어가 *마스킹되지 않습니다*.

### <a name="display-form"></a>표시 형식

문장 부호 및 대문자 표시는 텍스트를 더 쉽게 이해할 수 있도록 강조할 위치, 일시 중지할 위치 등을 알려줍니다. 표시 형식은 문장 부호 및 대문자 표시를 인식 결과에 추가하여 음성 텍스트를 표시하는 애플리케이션에 가장 적합한 형식입니다.

표시 형식은 마스킹된 ITN 형식을 확장하므로 불경한 언어 매개 변수 값을 `masked` 또는 `raw`로 설정할 수 있습니다. 값을 `raw`로 설정하면 사용자가 말하는 모든 불경한 언어가 인식 결과에 포함됩니다. 값을 `masked`로 설정하면 언어의 불경한 어휘집에 속한 것으로 인식되는 단어가 별표로 바뀝니다.

### <a name="sample-responses"></a>샘플 응답

모든 페이로드는 JSON 구조입니다.

`simple` 구 결과의 페이로드 형식:

```json
{
  "RecognitionStatus": "Success",
  "DisplayText": "Remind me to buy 5 pencils.",
  "Offset": "1236645672289",
  "Duration": "1236645672289"
}
```

`detailed` 구 결과의 페이로드 형식:

```json
{
  "RecognitionStatus": "Success",
  "Offset": "1236645672289",
  "Duration": "1236645672289",
  "NBest": [
      {
        "Confidence" : "0.87",
        "Lexical" : "remind me to buy five pencils",
        "ITN" : "remind me to buy 5 pencils",
        "MaskedITN" : "remind me to buy 5 pencils",
        "Display" : "Remind me to buy 5 pencils.",
      },
      {
        "Confidence" : "0.54",
        "Lexical" : "rewind me to buy five pencils",
        "ITN" : "rewind me to buy 5 pencils",
        "MaskedITN" : "rewind me to buy 5 pencils",
        "Display" : "Rewind me to buy 5 pencils.",
      }
  ]
}
```

## <a name="profanity-handling-in-speech-recognition"></a>음성 인식의 불경한 언어 처리

Microsoft Speech Service는 많은 사람들이 "불경한 언어"로 분류하는 단어와 구를 포함하여 모든 형태의 사람 음성을 인식합니다. 서비스에서 *profanity* 쿼리 매개 변수를 사용하여 불경한 언어를 처리하는 방법을 제어할 수 있습니다. 기본적으로 서비스는 *speech.phrase* 결과에 불경한 언어를 표시하지 않으며, 불경한 언어가 포함된 *speech.hypothesis* 메시지도 반환하지 않습니다.

| *profanity* 값 | 설명 |
| - | - |
| `masked` | 불경한 언어를 별표로 마스킹합니다. 이 동작은 기본값입니다. |
| `removed` | 모든 결과에서 불경한 언어를 제거합니다. |
| `raw` | 모든 결과에서 불경한 언어를 인식하고 반환합니다. |

### <a name="profanity-value-masked"></a>`Masked` profanity 값

불경한 언어를 마스킹하려면 *profanity* 쿼리 매개 변수를 *masked* 값으로 설정합니다. *profanity* 쿼리 매개 변수에 이 값이 있거나 요청에 대해 지정되지 않으면 서비스에서 불경한 언어를 *마스킹합니다*. 서비스에서 인식 결과의 불경한 언어를 별표로 바꾸면서 마스킹을 수행합니다. 불경한 언어 마스킹 처리를 지정하면 서비스에서 불경한 언어가 포함된 *speech.hypothesis* 메시지를 반환하지 않습니다.

### <a name="profanity-value-removed"></a>`Removed` profanity 값

*profanity* 쿼리 매개 변수의 값이 *removed*이면 서비스에서 *speech.phrase* 및 *speech.hypothesi* 메시지 모두로부터 불경한 언어를 제거합니다. 결과는 *불경한 언어를 말하지 않은 경우*와 같습니다.

#### <a name="profanity-only-utterances"></a>불경한 언어 전용 발화

애플리케이션에서 불경한 언어를 제거하도록 서비스를 구성한 경우 사용자는 *불경한 언어만* 말할 수 있습니다. 이 시나리오의 경우 인식 모드가 *받아쓰기* 또는 *대화*이면 서비스에서 *speech.result*를 반환하지 않습니다. 인식 모드가 *대화형*이면 서비스에서 상태 코드가 *NoMatch*인 *speech.result*를 반환합니다.

### <a name="profanity-value-raw"></a>`Raw` profanity 값

*profanity* 쿼리 매개 변수의 값이 *raw*이면 서비스에서 *speech.phrase* 또는 *speech.hypothesis*의 불경한 언어를 제거하거나 마스킹하지 않습니다.
