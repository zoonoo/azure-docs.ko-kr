---
title: 실습생 모드 - Personalizer
description: 코드 변경 없이 실습생 모드를 사용하여 모델에 대한 자신감을 쌓는 방법을 알아봅니다.
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 05/01/2020
ms.openlocfilehash: 531917d9c48915f71354b4cd35747ecd9d33a6f8
ms.sourcegitcommit: f28ebb95ae9aaaff3f87d8388a09b41e0b3445b5
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/30/2021
ms.locfileid: "100385033"
---
# <a name="use-apprentice-mode-to-train-personalizer-without-affecting-your-existing-application"></a>기존 애플리케이션에 미치는 영향 없이 실습생 모드로 Personalizer 학습

**실제** 보충 학습에서의 특성 때문에, Personalizer 모델은 프로덕션 환경에서만 교육할 수 있습니다. 모델의 충분한 학습에는 시간이 소요되므로 새 사용 사례를 배포할 때는 Personalizer 모델이 효율적으로 수행되지 않습니다.  **실습생 모드** 를 통해 이러한 상황을 완화하고 개발자의 코드 변경 없이도 모델에 대한 자신감을 쌓을 수 있습니다.

[!INCLUDE [Important Blue Box - Apprentice mode pricing tier](./includes/important-apprentice-mode.md)]

## <a name="what-is-apprentice-mode"></a>실습생 모드란?

실습생이 마스터에게 배우면서 경험을 통해 점차 나아지듯이, 기존 애플리케이션 로직에서 얻은 결과를 Personalizer가 관찰하여 학습할 수 있게 하는 _동작_ 입니다.

Personalizer는 애플리케이션과 동일한 출력을 모방하여 학습합니다. 이벤트 흐름이 많을수록 Personalizer가 기존 로직이나 결과에 미치는 영향 없이 기존 애플리케이션을 _따라잡을_ 수 있습니다. Azure Portal 및 API에서 사용할 수 있는 메트릭은 모델의 학습 성과를 이해하는 데 도움이 됩니다.

Personalizer가 학습을 통해 일정 수준의 이해를 확보한 후에는 개발자가 동작을 실습생 모드에서 온라인 모드로 변경할 수 있습니다. 이 시점이 되면 Personalizer가 Rank API의 작업에 영향을 미치기 시작합니다.

## <a name="purpose-of-apprentice-mode"></a>실습생 모드의 용도

실습생 모드는 Personalizer 서비스 및 기계 학습 기능을 신뢰하고, 서비스가 온라인 트래픽에 위험을 초래하지 않고 학습 가능한 정보를 보내고 있음을 재확인할 수 있게 합니다.

실습생 모드를 사용하는 두 가지 주요 이유는 다음과 같습니다.

* **콜드 부팅** 완화: 실습생 모드는 최상의 조치를 반환하지 않고 약 60-80%의 만족스러운 수준의 효과를 달성하지 못하는 경우 "새로운" 모델의 학습 시간을 관리하고 평가하는 데 도움이 됩니다.
* **작업 및 컨텍스트 기능 유효성 검사**: 작업 및 컨텍스트에서 보낸 기능은 부적절하거나 부정확할 수 있습니다. Personalizer가 이상적인 효과 수준을 획득하기에는 너무 적거나, 과다하거나, 부정확하거나, 너무 특정할 수 있습니다. [기능 평가](concept-feature-evaluation.md)를 사용하여 기능 관련 문제를 찾아 해결할 수 있습니다.

## <a name="when-should-you-use-apprentice-mode"></a>실습생 모드를 사용해야 하는 경우

Personalizer의 영향을 받지 않는 사용자의 환경은 그대로 두고 다음 시나리오를 통해 Personalizer를 학습시켜 효과를 증대하기 위해 실습생 모드를 사용할 수 있습니다.

* 새 사용 사례에서 Personalizer을 구현합니다.
* 컨텍스트 또는 작업에서 보내는 기능을 크게 변경했습니다.
* 보상을 계산하는 방법과 방법이 크게 변경되었습니다.

실습생 모드는 Personalizer가 보상 점수에 미치는 영향을 측정하는 효과적인 방법이 아닙니다. 각 Rank 호출에 대해 가능한 최적의 작업을 선택하는 데 Personalizer가 어떻게 효과가 있는지 측정하기 위해 [오프라인 평가](concepts-offline-evaluation.md)를 사용합니다.

## <a name="who-should-use-apprentice-mode"></a>실습생 모드를 사용해야 하는 사람

실습생 모드는 개발자, 데이터 과학자 및 비즈니스 의사 결정자에게 유용합니다.

* **개발자** 는 실습생 모드를 통해 Rank 및 Reward API가 애플리케이션에서 정확하게 사용되고 있고, 애플리케이션에서 Personalizer에 보내는 기능이 버그나 무관한 기능(예: 타임스탬프, 사용자 ID 요소)이 포함되지 않음을 확인할 수 있습니다.

* **데이터 과학자** 는 실습생 모드를 사용하여 기능이 Personalizer 모델 교육에 효과적이며, 보상 대기 시간이 너무 길거나 짧지 않은지 유효성을 검사할 수 있습니다.

* **비즈니스 의사 결정자** 는 실습생 모드를 통해, 기존 비즈니스 논리와 비교하여 결과(즉, 보상)를 향상시키는 데 있어 Personalizer의 가능성을 평가할 수 있습니다. 이렇게 하면 실제 매출과 사용자 만족을 좌우하는 사용자 환경에 영향이 있는 합리적 의사 결정을 내릴 수 있습니다.

## <a name="comparing-behaviors---apprentice-mode-and-online-mode"></a>동작 비교 - 실습생 모드 및 온라인 모드

다음과 같은 방법으로 실습생 모드와 온라인 모드의 차이를 학습합니다.

|영역|실습생 모드|온라인 모드|
|--|--|--|
|사용자 경험에 미치는 영향|기존 사용자 동작을 사용하여, Personalizer가 **기본 작업** 이 어땠을 것이며 받게 되는 보상은 무엇인지를 관찰만(영향 없음) 하도록 하여 교육할 수 있습니다. 즉, 사용자의 경험과 그에 따른 비즈니스 결과에는 영향이 없습니다.|사용자 동작에 영향을 주는, Rank 호출이 반환한 주요 작업을 표시합니다.|
|학습 속도|Personalizer는 온라인 모드에서 학습할 때보다 실습생 모드에서의 학습이 더 느립니다. 실습생 모드는 **기본 작업** 에서 얻은 보상을 관찰하는 것으로만 학습할 수 있는데, 탐색을 수행할 수 없기 때문에 학습 속도가 제한됩니다.|현재 모델을 활용하고 새 추세를 살펴볼 수 있으므로 더 빠르게 학습합니다.|
|학습 효과 "천장"|Personalizer는 근접하지만 거의 일치하지 않으며 기본 비즈니스 논리의 성능(각 순위 호출의 **기본 작업** 에 의해 달성된 보상 합계)을 초과할 수 없습니다. 이 근사치 한도는 탐색만큼 줄어듭니다. 예를 들어 20% 탐색에서는 실습생 모드 성능이 80%를 초과할 가능성이 매우 낮고, 60%면 마치고 온라인 모드로 이동할 수 있는 합당한 목표입니다.|Personalizer는 애플리케이션 기준을 초과해야 하며, 지속적 모델 향상을 위해 시간 경과에 따라 중단 지점에서 오프라인 평가 및 기능 평가를 수행해야 합니다. |
|rewardActionId에 대한 Rank API 값|_rewardActionId_ 는 항상 Rank 요청에서 전송하는 첫 번째 작업이므로 사용자 환경에는 영향이 없습니다. 즉, 실습생 모드인 동안은 Rank API가 애플리케이션에서 보이지 않습니다. 애플리케이션의 Reward API가 서로 다른 모드 간에 Reward API를 사용하는 방법을 바꿔서는 안 됩니다.|사용자 환경은 Personalizer가 애플리케이션에 대해 선택하는 _rewardActionId_ 가 변경합니다. |
|평가|Personalizer는 기본 비즈니스 논리가 가져오는 보상 합계와 해당 시점의 온라인 모드에서 Personalizer가 가져올 수 있는 보상 합계를 비교합니다. 리소스에 대한 비교는 Azure Portal에서 사용할 수 있습니다.|[오프라인 평가](concepts-offline-evaluation.md)를 실행하여 Personalizer의 효과를 평가합니다. 이렇게 애플리케이션 기준의 잠재적 보상과 대비하여 Personalizer가 달성한 총 보상을 비교할 수 있습니다.|

실습생 모드의 효과에 대한 참고:

* 실습생 모드에서의 Personalizer 효과는 애플리케이션 기준에 100% 가까이 가는 경우가 거의 없고 이를 초과할 수도 없습니다.
* 모범 사례는 100% 달성을 시도하지 않는 것입니다. 사용 사례에 따라 60% – 80%범위를 목표로 해야 합니다.

## <a name="using-apprentice-mode-to-train-with-historical-data"></a>실습생 모드에서 과거 데이터로 학습

양이 많은 과거 데이터로 Personalizer를 교육하려는 경우 실습생 모드를 사용하여 Personalizer에서 데이터를 재생할 수 있습니다.

Personalizer를 실습생 모드로 설정하고 과거 데이터의 작업과 컨텍스트 기능으로 Rank를 호출하는 스크립트를 만듭니다. 이 데이터의 레코드 계산을 기준으로 Reward API를 호출합니다. 어느 정도의 결과를 보려면 약 50,000개의 과거 이벤트가 필요하지만 결과에 대한 신뢰를 높이려면 500,000개가 권장됩니다.

과거 데이터로부터 학습할 때는 컨텍스트 및 작업에 대한 기능, Rank 요청에 사용되는 JSON의 레이아웃, 이 학습 데이터 세트의 보상 계산에서 보내는 데이터가, 기존 애플리케이션에서 사용할 수 있는 데이터(보상의 기능 및 계산)와 일치해야 합니다.

오프라인 및 사후 데이터는 불완전하고 더 지저분한 경향이 있고 형식이 다릅니다. 과거 데이터를 통한 학습은 가능하나 그에 따른 결과는 결정적이지 못하고, Personalizer가 얼마나 잘 학습할 것인지 예측하기 위한 지표는 되지 못할 수 있습니다. 특히 과거 데이터와 기존 애플리케이션 사이에 기능 차이가 있는 경우가 그렇습니다.

일반적으로 과거 데이터를 통한 학습과 비교할 때 Personalizer에서는 실습생 모드로 변경하고 기존 애플리케이션으로부터 학습하는 것이 더 적은 노력, 데이터 엔지니어링 및 정리 작업으로 효과적인 모델을 구현할 수 있는 더 효과적인 방법이 됩니다.

## <a name="using-apprentice-mode-versus-ab-tests"></a>실습생 모드와 A/B 테스트 사용 비교

유효성을 검사했고 온라인 모드에서 학습 중인 경우에만 Personalizer 처리의 A/B 테스트가 유용합니다. 실습생 모드에서는 **기본 동작** 만 사용하므로 모든 사용자가 제어 환경을 효과적으로 볼 수 있습니다.

Personalizer가 _처리_ 에 한정된 경우라도, Personalizer 학습에 적합한 데이터인지에 대한 유효성을 검사할 때는 같은 어려움이 있습니다. 대신 100% 트래픽과, 제어(영향을 받지 않음) 환경을 가져오는 모든 사용자로 실습생 모드를 사용할 수 있습니다.

Personalizer를 사용하고 온라인으로 학습하는 사용 사례가 갖추어지면, 보상에 사용한 신호보다 더 복잡할 수 있는 결과의 과학적 비교와 제어되는 코호트를 A/B 실험을 통해 수행할 수 있습니다. A/B 테스트에서 답할 수 있는 예제 질문은 `In a retail website, Personalizer optimizes a layout and gets more users to _check out_ earlier, but does this reduce total revenue per transaction?`입니다.

## <a name="next-steps"></a>다음 단계

* [활성 및 비활성 이벤트](concept-active-inactive-events.md) 알아보기
