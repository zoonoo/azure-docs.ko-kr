---
title: Azure Vm에 대 한 TCP/IP 성능 튜닝 | Microsoft Docs
description: 관계는 Azure Vm에 다양 한 일반적인 TCP/IP 성능 튜닝 기술에 알아봅니다.
services: virtual-network
documentationcenter: na
author:
- rimayber
- dgoddard
- stegag
- steveesp
- minale
- btalb
- prachank
manager: paragk
editor: ''
ms.assetid: ''
ms.service: virtual-network
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: infrastructure-services
ms.date: 04/02/2019
ms.author:
- rimayber
- dgoddard
- stegag
- steveesp
- minale
- btalb
- prachank
ms.openlocfilehash: 1e8605a41cbe610c971b891309b2149d221b8b27
ms.sourcegitcommit: 3102f886aa962842303c8753fe8fa5324a52834a
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 04/23/2019
ms.locfileid: "61032454"
---
# <a name="tcpip-performance-tuning-for-azure-vms"></a>Azure Vm에 대 한 튜닝 TCP/IP 성능

이 문서에서는 일반적인 TCP/IP 성능 튜닝 기술 및 Azure에서 실행 중인 virtual machines에 대 한 사용 하는 경우를 고려해 야 할 몇 가지를 설명 합니다. 기술의 기본 개요를 제공 하 고 튜닝 하는 방법을 탐색 합니다.

## <a name="common-tcpip-tuning-techniques"></a>일반 TCP/IP 튜닝 기법

### <a name="mtu-fragmentation-and-large-send-offload"></a>MTU, 조각화 및 large send offload

#### <a name="mtu"></a>MTU

최대 전송 단위 (MTU)는 네트워크 인터페이스를 통해 보낼 수 있는 바이트 단위로 지정 된 최대 크기 프레임 (패킷)입니다. MTU 구성 가능한 설정입니다. Azure Vm에서 기본 MTU 사용 되며 대부분의 네트워크 장치에 대 한 기본 설정은 전역적으로 1,500 바이트.

#### <a name="fragmentation"></a>조각화

조각화는 네트워크 인터페이스의 MTU를 초과 하는 패킷을 보낼 때 발생 합니다. TCP/IP 스택을 인터페이스의 MTU를 준수 하는 작은 조각 (조각)으로 패킷을 연결이 끊어집니다. 조각화 IP 계층에서 발생 하 고 기본 프로토콜 (예: TCP)와 무관 합니다. 2,000 바이트 패킷이 MTU 인 1500 사용 하 여 네트워크 인터페이스를 통해 전송 되 면 패킷 1,500 바이트 패킷이 하나 및 하나 500 바이트 패킷으로 나눌 수 됩니다.

원본과 대상 간의 경로에 네트워크 장치 MTU를 초과 하거나 패킷을 더 작은 부분으로 조각 중 놓기 패킷의 수 있습니다.

#### <a name="the-dont-fragment-bit-in-an-ip-packet"></a>Don't Fragment IP 패킷에 비트가

없는 조각 (DF) 비트가 IP 프로토콜 헤더에 있는 플래그입니다. DF 비트 발신자와 수신자 간의 경로에서 네트워크 장치 패킷의 조각화 하지 해야 나타냅니다. 여러 가지 이유로이 비트를 설정할 수 있습니다. (한 가지 예에 대 한이 문서의 "Path MTU Discovery" 섹션 참조). 네트워크 장치 Don't Fragment 비트 집합을 사용 하 여 패킷을 받고 해당 패킷을 초과 하는 장치 인터페이스의 MTU, 표준 동작은 패킷을 삭제 하는 장치에 대 한 것입니다. 장치를 다시 패킷의 원본 ICMP 조각화 필요한 메시지를 보냅니다.

#### <a name="performance-implications-of-fragmentation"></a>조각화의 성능에 미치는 영향

조각화 음수 성능 영향을 미칠 수 있습니다. 성능에 미치는 영향에 대 한 주된 이유 중 하나에 조각화의 p U/메모리 영향 및 패킷 리어셈블리입니다. 패킷을 조각화 해야 하는 네트워크 장치를 조각화 하는 데 p U/메모리 리소스를 할당 해야 합니다.

동일한 작업에는 패킷을 다시 조합 하는 경우 발생 합니다. 네트워크 장치를 원래 패킷이으로 어셈블할 수 있도록 받을 때까지 모든 부분을 저장 해야 합니다. 조각화 및 리어셈블리가이 프로세스는 대기 시간이 발생할 수 있습니다.

조각화의 다른 가능한 부정적인 영향은 조각화 된 패킷이 순서로 도착할 수 있습니다. 패킷 순서가 수신 되 고 일부 유형의 네트워크 장치 삭제할 수 있습니다. 이 경우 전체 패킷 재전송 될 해야 합니다.

조각 네트워크 방화벽과 같은 보안 장치에서 일반적으로 삭제 됩니다 또는 버퍼 고갈 되는 네트워크 장치의 수신 하는 경우. 네트워크 장치의 수신 버퍼를 모두 사용 했습니다., 네트워크 장치를 조각화 된 패킷이 다시 어셈블해야 하므로 하려고 하지만 리소스를 저장 하 고 패킷을 reassume 필요는 없습니다.

조각화는 인터넷을 통해 다양 한 네트워크를 연결 하는 경우 필요에 대 한 지원 않지만 음수 작업으로 조각화를 볼 수 있습니다.

#### <a name="benefits-and-consequences-of-modifying-the-mtu"></a>이점 및 MTU를 수정한 결과

일반적으로 말해 MTU를 늘려 더 효율적으로 네트워크를 만들 수 있습니다. 모든 패킷이 전송 되는 원래 패킷이에 추가 되는 헤더 정보에는 조각화 더 패킷을 만들 때 자세한 머리글 오버 헤드 되며 있도록 네트워크 효율성이 떨어집니다.

예제는 다음과 같습니다. 이더넷 헤더 크기는 14 바이트 + 프레임 일관성을 보장 하는 4 바이트 프레임 검사 순서입니다. 하나의 2,000 바이트 패킷이 전달 되 면 네트워크에서 18 바이트의 이더넷 오버 헤드 추가 됩니다. 패킷을 1,500 바이트 패킷이에 500 바이트 패킷 조각화 된 경우 각 패킷의 총 36 바이트 이더넷 헤더의 18 바이트를 해야 합니다.

MTU를 늘리면 더 효율적으로 네트워크를 만들 반드시 않습니다 있는 점을 염두에 두십시오. 응용 프로그램에는 500 바이트 패킷이 보내는 경우 동일한 머리글 오버 헤드는 MTU 1,500 바이트 또는 9,000 바이트 인지 여부를 존재 합니다. 네트워크는 MTU 영향을 받는 더 큰 패킷 크기를 사용 하는 경우에 효율적 됩니다.

#### <a name="azure-and-vm-mtu"></a>Azure 및 VM MTU

기본 Azure Vm에 대 한 MTU는 1,500 바이트입니다. Azure Virtual Network 스택 1,400 바이트로 패킷 조각을 하려고 합니다. 하지만 가상 네트워크 스택을 사용 하면 패킷 2,006 바이트까지 비트 IP 헤더에 설정 된 경우.

Vm의 1,500 MTU 없는 경우에 1,400 바이트에서 패킷을 조각화 때문에 가상 네트워크 스택는 기본적으로 비효율적인 아닙니다. 네트워크 패킷 비율을는 1,400 또는 1,500 바이트 보다 훨씬 작습니다.

#### <a name="azure-and-fragmentation"></a>Azure 및 조각화

"순서가 조각을" 원래 조각화 된 순서 대로 도착 하지 조각화 된 패킷이, 삭제 하려면 가상 네트워크 스택이 설정 됩니다. 이러한 패킷은 FragmentSmack 호출 2018 년 11 월에에서 발표 하는 네트워크 보안 취약점으로 인해 주로 삭제 됩니다.

FragmentSmack은 조각화 된 IPv4 및 IPv6 패킷 리어셈블리를 처리 하는 Linux 커널 방식이 결함입니다. 원격 공격자가이 결함 대상 시스템에 향상 된 CPU 및 서비스 거부가 발생할 수 있는 트리거 비용이 많이 드는 조각 리어셈블리 작업을 사용할 수 있습니다.

#### <a name="tune-the-mtu"></a>MTU를 조정 합니다.

다른 운영 체제의 경우와 Azure VM MTU를 구성할 수 있습니다. 위에서 설명한 Azure에서 발생 하는 조각화를 고려해 야 하지만 MTU를 구성 하는 경우.

VM Mtu을 늘리려면 고객을 좋습니다 하지 않습니다. 이 내용은 Azure MTU를 구현 하 고 조각화를 수행 하는 방법의 세부 정보를 설명 하기 위해 것입니다.

> [!IMPORTANT]
>MTU를 늘리면 성능을 향상 시키기 위해 알려질 하 고 응용 프로그램 성능에 부정적인 영향을 있을 수 있습니다.
>
>

#### <a name="large-send-offload"></a>큰 send 오프 로드

큰 send 오프 로드 (하도록 하므로 LSO) 이더넷 어댑터 패킷의 조각화를 오프 로드 하 여 네트워크 성능을 향상 시킬 수 있습니다. 하도록 하므로 LSO을 사용 하는 TCP/IP 스택을 큰 TCP 패킷을 만들고 전달 하기 전에 조각화에 대 한 이더넷 어댑터에 보냅니다. 하도록 하므로 LSO의 장점은 패킷이 MTU 따르며 하드웨어에서 수행 되는 이더넷 인터페이스에 해당 처리를 오프 로드 하는 크기로 배치 된 세그먼트화 CPU 사용 가능한 수 있습니다. 참조 하도록 하므로 LSO의 이점에 대해 자세히 알아보려면 [큰 송신을 지 원하는 오프 로드](https://docs.microsoft.com/windows-hardware/drivers/network/performance-in-network-adapters#supporting-large-send-offload-lso)합니다.

하도록 하므로 LSO 사용 하는 경우에 Azure 고객에 게 패킷 캡처를 수행 하는 경우 큰 프레임 크기에 표시 될 수입니다. 이러한 큰 프레임 크기는 조각화가 발생 하는 생각 하는 일부 고객 또는 대용량 MTU 사용 하 고 있음이 없을 때 발생할 수 있습니다. LSO를 사용 하 여 이더넷 어댑터를 더 큰 TCP 패킷을 만드는 TCP/IP 스택에 (MSS) 더 큰 최대 세그먼트 크기를 보급할 수입니다. 이 전체에서 분할 되지 않은 프레임 이더넷 어댑터에 전달 됩니다 및 VM에서 수행 하는 패킷 캡처에 표시 됩니다. 하지만 패킷을 분해할 수 많은 작은 프레임 이더넷 어댑터의 MTU에 따라 이더넷 어댑터에 의해.

### <a name="tcp-mss-window-scaling-and-pmtud"></a>TCP MSS 창 크기 조정 및가 PMTUD

#### <a name="tcp-maximum-segment-size"></a>TCP 최대 세그먼트 크기

TCP 최대 세그먼트 크기 (MSS)는 TCP 패킷의 조각화를 방지 하는 TCP 세그먼트의 크기를 제한 하는 설정입니다. 일반적으로 운영 체제 MSS를 설정 하려면 다음이 수식을 사용 됩니다.

`MSS = MTU - (IP header size + TCP header size)`

IP 헤더 및 TCP 헤더에는 각각 20 바이트 또는 총 40 바이트 수입니다. 따라서 MTU 인 1500 사용 하 여 인터페이스의 1,460 MSS 해야 합니다. 하지만 MSS를 구성할 수 있습니다.

이 설정은에 TCP 세 방향 핸드셰이크에서 동의 원본과 대상 간에 TCP 세션을 설정할 때. 양쪽 모두 MSS 값을 보내고 둘 중 더 작은 TCP 연결에 사용 됩니다.

원본 및 대상의 Mtu 없는 MSS 값을 결정 하는 유일한 요소를 염두에 두어야 합니다. Azure VPN Gateway를 포함 하 여 VPN gateway와 같은 중간 네트워크 장치에 최적의 네트워크 성능을 보장 하기 위해 MTU 원본 및 대상 독립적으로 조정할 수 있습니다.

#### <a name="path-mtu-discovery"></a>Path MTU Discovery

MSS 협상 됩니다 있지만 사용할 수 있는 실제 MSS를 나타내지 않을 수 있습니다. 즉, 원본과 대상 간의 경로에 다른 네트워크 장치 원본 및 대상 보다 낮은 MTU 값을 사용할 수 있습니다. 이 경우 해당 MTU 패킷을 보다 작은 장치에서 패킷을 삭제 합니다. 장치는 다시 해당 MTU를 포함 하는 ICMP 조각화 필요한 (유형 3, 코드 4) 메시지를 보낼 됩니다. 이 ICMP 메시지 원본 호스트를 경로 MTU를 적절 하 게 줄일 수 있습니다. 프로세스가 (Path MTU Discovery PMTUD) 라고 합니다.

가 PMTUD 프로세스 효율적 이며 네트워크 성능에 영향을 줍니다. 네트워크 경로 MTU를 초과 하는 패킷이 전송 될 때 패킷이 낮은 MSS를 사용 하 여 다시 전송 해야 해야 합니다. 보낸 사람에 게 하지 메시지가 ICMP 조각화 필요한, 아마도 경로에 네트워크 방화벽으로 인해 (일반적으로 라고는 *가 PMTUD 블랙홀*), 보낸 사람에 게는 MSS를 줄이고 지속적으로 할 알 수 없습니다 패킷을 다시 전송 합니다. 이 때문에 Azure VM MTU 증가 하는 것이 좋습니다.

#### <a name="vpn-and-mtu"></a>VPN 및 MTU

캡슐화 (예: IPsec Vpn)을 수행 하는 Vm을 사용 하면 패킷 크기와 MTU에 대 한 몇 가지 추가 고려 사항이 있습니다. Vpn는 패킷 크기를 증가 하며 작은 MSS 패킷이 추가 헤더를 추가 합니다.

Azure에 대 한 TCP MSS 고정 1,350 바이트로 설정 인터페이스 MTU 1400 터널링 하는 것이 좋습니다. 자세한 내용은 참조는 [VPN 장치 및 IPSec/IKE 매개 변수 페이지](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpn-devices)합니다.

### <a name="latency-round-trip-time-and-tcp-window-scaling"></a>대기 시간, 왕복 시간 및 TCP 창 크기 조정

#### <a name="latency-and-round-trip-time"></a>대기 시간 및 왕복 시간

네트워크 대기 시간 속도의 광섬유 파이버 네트워크를 통해 적용 됩니다. 두 개의 네트워크 장치 간의 왕복 시간 (RTT) tcp 네트워크 처리량도 효과적으로 적용 됩니다.

| | | | |
|-|-|-|-|
|**Route**|**거리**|**단방향 시간**|**RTT**|
|San Francisco에 New York|4,148 km|21 ms|42 ms|
|런던에 New York|5,585 km|28ms|56 ms|
|시드니에 New York|15,993 km|80ms|160 밀리초|

이 테이블에는 두 위치 사이의 직선 거리를 보여 줍니다. 네트워크에서 거리는 직선 거리 보다 일반적으로 깁니다. 광원의 속도 따라 처럼 최소 RTT를 계산 하기 위해 간단한 수식을 다음과 같습니다.

`minimum RTT = 2 * (Distance in kilometers / Speed of propagation)`

200 속도 전파를 사용할 수 있습니다. 미터, light 1 밀리초에서 전송 되는 거리입니다.

예로 샌프란시스코에 뉴욕 보겠습니다. 직선 거리가 4,148 km를 보여 줍니다. 수식에 해당 값을 연결,에서는 다음과 같은 이점을 누릴

`Minimum RTT = 2 * (4,148 / 200)`

수식의 출력은 밀리초에서입니다.

네트워크 성능을 극대화 하려면 해당 사이의 최단 거리를 사용 하 여 대상을 선택 논리 옵션이입니다. 또한 트래픽 경로 최적화 하 고 대기 시간을 줄이고 가상 네트워크를 디자인 해야 합니다. 자세한 내용은이 문서의 "네트워크 디자인 고려 사항" 섹션을 참조 하세요.

#### <a name="latency-and-round-trip-time-effects-on-tcp"></a>대기 시간 및 왕복 시간이 미치는 TCP

왕복 시간 최대 TCP 처리량에 직접적인 영향을 미칩니다. TCP 프로토콜에서 *창 크기* 최대 보낸 승인 수신기에서 수신 해야 하기 전에 TCP 연결을 통해 보낼 수 있는 트래픽 양입니다. TCP MSS 1,460로 설정 된 경우 TCP 창 크기가 65,535로 설정 됩니다 보낸 사람에 게 승인 수신기에서 수신 하도록 설정 되기 전에 45 패킷을 보낼 수 있습니다. 보낸 사람에 게 승인 하지 경우, 데이터를 다시 전송 합니다. 수식은 다음과 같습니다.

`TCP window size / TCP MSS = packets sent`

이 예제에서는 65,535 1,460 반올림 됩니다 / 최대 45.

이 "승인을 기다리는 중" 상태에서 데이터를 안정적으로 배달할 수 있는 메커니즘은 TCP 처리량에 영향을 주도록 RTT 원인 보낸 사람에 게 승인을 기다리는 오래, 오래 더 많은 데이터를 보내기 전에 대기 해야 합니다.

단일 TCP 연결의 최대 처리량 공식은 다음과 같습니다.

`Window size / (RTT latency in milliseconds / 1,000) = maximum bytes/second`

이 표에서 최대 메가바이트 / 초당 단일 TCP 연결의 처리량입니다. (가독성을 위해 메가바이트 사용 됨 측정 단위입니다.)

| | | | |
|-|-|-|-|
|**TCP 창 크기 (바이트)**|**RTT 대기 시간 (밀리초)**|**최대 메가바이트 초당 처리량**|**최대 메가 비트/초 처리량**|
|65,535|1|65.54|524.29|
|65,535|30|2.18|17.48|
|65,535|60|1.09|8.74|
|65,535|90|.73|5.83|
|65,535|120|.55|4.37|

패킷 손실 되 면 보낸 사람에 게 이미 전송 되는 데이터를 다시 전송 하는 동안 TCP 연결의 최대 처리량 줄어들게 됩니다.

#### <a name="tcp-window-scaling"></a>TCP 창 크기 조정

TCP 창 크기 조정을 승인을 요구 하기 보낼 데이터가 더 이상 허용 하도록 TCP 창 크기를 동적으로 증가 하는 기술입니다. 이전 예에서 45 패킷은 보낼 전에 승인 필요 했습니다. 승인 필요 하기 전에 보낼 수 있는 패킷의 수를 늘리면 보낸 TCP 최대 처리량을 증가 시킬 승인을 기다리는 횟수를 감소 합니다.

이 테이블은 이러한 관계를 보여 줍니다.

| | | | |
|-|-|-|-|
|**TCP 창 크기 (바이트)**|**RTT 대기 시간 (밀리초)**|**최대 메가바이트 초당 처리량**|**최대 메가 비트/초 처리량**|
|65,535|30|2.18|17.48|
|131,070|30|4.37|34.95|
|262,140|30|8.74|69.91|
|524,280|30|17.48|139.81|

하지만 TCP 창 크기에 대 한 TCP 헤더 값이만 2 바이트 길이의 수신 창에 대 한 최대값은 65535입니다. 최대 창 크기를 늘리려면 TCP 창 크기 조정 비율을 도입 되었습니다.

배율 인수는 운영 체제에서 구성할 수 있는 설정 이기도 합니다. TCP 창 크기를 배율 인수를 사용 하 여 공식은 다음과 같습니다.

`TCP window size = TCP window size in bytes \* (2^scale factor)`

다음은 3과 65,535의 창 크기를 창 배율 인수에 대 한 계산이입니다.

`65,535 \* (2^3) = 262,140 bytes`

14 (허용 된 최대 오프셋)의 TCP 창 크기에 14 결과 배율 인수입니다. TCP 창 크기를 1,073,725,440 바이트 (8.5 기가 비트) 됩니다.

#### <a name="support-for-tcp-window-scaling"></a>TCP 창 크기 조정에 대 한 지원

Windows는 다른 연결 형식에 대 한 다른 배율 인수를 설정할 수 있습니다. (연결 클래스를 포함 데이터 센터, 인터넷 및 등) 사용 된 `Get-NetTCPConnection` 연결 형식 크기 조정 창을 보려면 PowerShell 명령:

```powershell
Get-NetTCPConnection
```

사용할 수는 `Get-NetTCPSetting` 각 클래스의 값을 보려면 PowerShell 명령:

```powershell
Get-NetTCPSetting
```

설정할 수 있습니다 초기 TCP 창 크기 및 TCP에 대 한 배율 정보를 Windows에서 사용 하 여는 `Set-NetTCPSetting` PowerShell 명령입니다. 자세한 내용은 [집합 NetTCPSetting](https://docs.microsoft.com/powershell/module/nettcpip/set-nettcpsetting?view=win10-ps)합니다.

```powershell
Set-NetTCPSetting
```

이러한 설정은 효과적인 TCP에 대 한 `AutoTuningLevel`:

| | | | |
|-|-|-|-|
|**AutoTuningLevel**|**배율**|**승수를 크기 조정**|**수식을<br/>최대 창 크기를 계산 합니다.**|
|사용 안 함|없음|없음|창 크기|
|제한|4|2^4|창 크기 * (2 ^4)|
|매우 제한|2|2^2|창 크기 * (2 ^2)|
|정상|8|2^8|창 크기 * (2 ^8)|
|실험적|14|2^14|창 크기 * (2 ^14)|

이러한 설정은 가능성이 가장 TCP 성능에 영향을 하지만 Azure의 컨트롤 외부에서 인터넷을 통해 다른 요인 TCP 성능 영향을 줄 수도 있습니다.

#### <a name="increase-mtu-size"></a>MTU 크기 늘리기

더 큰 MTU 큰 MSS 의미 하기 때문에 있는지 여부를 MTU를 늘려 성능을 향상 시킬 수 TCP 궁금할 수 있습니다. 그렇지 않을 가능성이 있습니다. TCP 트래픽 방금 초과 패킷 크기로 장단점이 있습니다. 앞에서 설명 했 듯이 TCP 처리량 성능에 영향을 주는 가장 중요 한 요소 이며 TCP 창 크기, 패킷 손실 및 RTT

> [!IMPORTANT]
> Azure 고객은 virtual machines에서 MTU 기본값을 변경 하지 것이 좋습니다.
>
>

### <a name="accelerated-networking-and-receive-side-scaling"></a>가속화 된 네트워킹 및 수신측 배율

#### <a name="accelerated-networking"></a>가속된 네트워킹

가상 머신 네트워크 함수는 CPU를 많이 VM 게스트와 하이퍼바이저/호스트에서 지금까지 되었습니다. 호스트를 통해 자신을 모든 패킷이 CPU, 모든 가상 네트워크 캡슐화 및 캡슐화를 포함 하 여 호스트에서 소프트웨어에서 처리 됩니다. 호스트를 통해 이동 하는 더 많은 트래픽을, 있도록 높을수록 CPU 로드 합니다. 호스트 CPU에 다른 작업을 사용 하 여 사용 중인 경우는 또한에 영향을 주는 네트워크 처리량 및 대기 시간을 Azure 가속된 네트워킹을 사용 하 여이 문제를 해결합니다.

가속화 된 네트워킹 SR-IOV와 같은 기술과 Azure의 내부 프로그래밍 가능한 하드웨어를 통해 일관 된 ultralow 네트워크 대기 시간을 제공합니다. 가속화 된 네트워킹에는 Azure 소프트웨어 정의 네트워킹 스택 Cpu off 및 FPGA 기반 SmartNICs에 많은 이동합니다. 이 변경 대기 시간 불일치 및 지터 감소 VM에서 부하를 줄이고를 저장 하는 계산 주기를 회수 하기 위해 최종 사용자 응용 프로그램을 수 있습니다. 즉, 성능이 보다 명확한 수 있습니다.

가속화 된 네트워킹에는 게스트 VM이 호스트를 우회 하 고 호스트의 SmartNIC 사용 하 여 직접 데이터를 설정 함으로써 성능이 향상 됩니다. 가속화 된 네트워킹의 몇 가지 이점은 다음과 같습니다.

- **더 낮은 대기 시간 / 더 높은 초당 패킷 초 (pps)**: 데이터 경로에서 가상 스위치를 제거 패킷이 정채 처리를 위해 호스트에서 소요 되는 시간을 제거 하 고 VM에서 처리할 수 있는 패킷의 수를 늘립니다.

- **감소 된 지터**: 가상 스위치 처리 적용 해야 하는 정책의 양과 처리를 수행 하는 CPU의 워크 로드에 따라 달라 집니다. 정책 적용을 하드웨어로 오프 로드 패킷을 호스트-에-VM 통신과 모든 소프트웨어 인터럽트 및 컨텍스트 전환 없이 VM에 직접 제공 하 여 해당 가변성을 제거 합니다.

- **CPU 사용률 감소**: 이 기능을 사용할 때는 호스트에서 가상 스위치를 바이패스하므로 네트워크 트래픽 처리에 CPU를 더 적게 사용하게 됩니다.

가속화 된 네트워킹을 사용 하려면 명시적으로 적절 한 각 VM에서 사용 하도록 설정 해야 합니다. 참조 [가속 네트워킹을 사용 하 여 Linux 가상 머신 만들기](https://docs.microsoft.com/azure/virtual-network/create-vm-accelerated-networking-cli) 지침에 대 한 합니다.

#### <a name="receive-side-scaling"></a>수신측 배율

수신 측 배율 (RSS)는 네트워크 드라이버 기술을 배포 하 여 네트워크 트래픽의 수신을 보다 효율적으로 배포 하는 다중 프로세서 시스템에서 여러 Cpu에서 처리를 수신 합니다. 간단히 말해 RSS 하나가 아닌 모든 사용 가능한 Cpu를 사용 하기 때문에 더 수신된 트래픽을 처리 하는 시스템 수 있습니다. 에 대 한 보다 기술적인 설명은 RSS 참조 하세요 [수신측 배율 소개](https://docs.microsoft.com/windows-hardware/drivers/network/introduction-to-receive-side-scaling)합니다.

가속화 된 네트워킹을 VM에서 사용 하는 경우 최상의 성능을 얻으려면, RSS를 사용 하도록 설정 해야 합니다. RSS는 가속화 된 네트워킹을 사용 하지 않는 Vm에 혜택을 제공할 수도 있습니다. RSS를 사용할 수 있는지 확인 하는 방법 및 사용 하도록 설정 하는 방법의 개요를 참조 하세요 [Azure virtual machines에 대 한 네트워크 처리량 최적화](http://aka.ms/FastVM)합니다.

### <a name="tcp-timewait-and-timewait-assassination"></a>TCP TIME_WAIT 및 TIME_WAIT 암살

TCP TIME_WAIT은 네트워크 및 응용 프로그램 성능에 영향을 주는 또 다른 일반적인 설정입니다. 열고 여러 소켓, 클라이언트 또는 TCP의 정상 작동 하는 동안 서버 (원본 IP:Source 포트 + 대상 IP:Destination 포트)으로 사용 중인 Vm에 지정 된 소켓에에서 놓일 수도 TIME_WAIT 상태 오랜 시간 동안입니다. 추가 데이터를 닫기 전에 소켓에 배달할 수 있도록 TIME_WAIT 상태 것입니다. 따라서 TCP/IP 스택을 일반적으로 자동으로 클라이언트의 TCP SYN 패킷을 삭제 하 여 소켓의 재사용을 방지 합니다.

소켓 TIME_WAIT 상태인 시간의 용량은 구성할 수 있습니다. 30 초에서 다양 한 범위가 240 초에 있습니다. 소켓은 한정 된 리소스 이며 언제 든 지 사용할 수 있는 소켓 수가 구성할 수 있습니다. (사용 가능한 소켓 수가 일반적으로 약 30,000 개입니다.) 사용 가능한 소켓을 사용 하는 경우, 클라이언트 및 서버에는 일치 하지 않는 TIME_WAIT 설정 하 고 VM가 TIME_WAIT 상태인 소켓을 다시 사용 하려고 하는 경우에 TCP SYN 패킷을 자동으로 새 연결이 실패 합니다.

아웃 바운드 소켓이 대 한 포트 범위에 대 한 값은 일반적으로 운영 체제의 TCP/IP 스택 내에서 구성할 수 있습니다. 동일한 작업 TCP TIME_WAIT 설정과 소켓 재사용 마찬가지입니다. 이러한 번호를 변경 하는 경우에 확장성을 향상 될 수 있습니다. 그러나 상황에 따라 이러한 변경 내용을 상호 운용성 문제가 발생할 수 있습니다. 이러한 값을 변경 하는 경우에 주의 해야 합니다.

이 크기 조정 제한을 해결 하기 위해 TIME_WAIT 암살을 사용할 수 있습니다. TIME_WAIT 암살 소켓을을 새 연결의 IP 패킷에 시퀀스 번호는 이전 연결에서 마지막 패킷의 시퀀스 번호를 초과 하는 경우와 같은 특정 상황에서 다시 사용할 수 있습니다. 이 경우 운영 체제는 새 연결을 설정 하면 (새 SYN/ACK 허용) 강제 TIME_WAIT 상태에 있던 이전 연결을 닫습니다. 이 기능은 Azure에서 Windows Vm에서 지원 됩니다. 다른 Vm에 대 한 지원에 대 한 자세한 OS 공급 업체를 사용 하 여 확인 합니다.

원본 포트 범위 및 TCP TIME_WAIT 설정을 구성 하는 방법에 대 한 자세한 참조 [네트워크 성능 향상을 위해 수정할 수 있는 설정을](https://docs.microsoft.com/biztalk/technical-guides/settings-that-can-be-modified-to-improve-network-performance)합니다.

## <a name="virtual-network-factors-that-can-affect-performance"></a>성능에 영향을 줄 수 있는 가상 네트워크 요소

### <a name="vm-maximum-outbound-throughput"></a>VM의 최대 아웃 바운드 처리량

Azure는 다양 한 VM 크기 및 유형, 다양 한 성능 기능을 사용 하 여 각각을 제공합니다. 이러한 기능 중 하나는 네트워크 처리량 (또는 대역폭)는 초당 메가 비트 (Mbps)로 측정 됩니다. 가상 컴퓨터는 공유 하드웨어에서 호스트 되므로, 네트워크 용량을 공평 하 게 동일한 하드웨어를 사용 하 여 가상 머신 간에 공유 해야 합니다. 더 큰 가상 컴퓨터에는 작은 가상 머신 보다 더 많은 대역폭이 할당 됩니다.

각 가상 머신에 할당된 네트워크 대역폭은 가상 머신에서의 송신(아웃바운드) 트래픽으로 측정됩니다. 가상 컴퓨터에서 나가는 모든 네트워크 트래픽은 대상에 관계없이 할당된 제한에 대해 계산됩니다. 예를 들어, 가상 컴퓨터를 1,000 1,000mbps 제한이 있으면 해당 제한 아웃 바운드 트래픽이 동일한 가상 네트워크 또는 Azure 외부의 다른 가상 컴퓨터에 대 한 대상이 인지 여부를 적용 합니다.

수신은 측정되거나 직접 제한되지 않습니다. 그러나 들어오는 데이터를 처리 하는 가상 머신의 기능이 영향을 줄 수 있는 CPU 및 저장소 제한 등의 다른 요소에 있습니다.

가속화 된 네트워킹은 대기 시간, 처리량 및 CPU 사용률을 포함 하 여 네트워크 성능을 향상 시키기 위해 설계 되었습니다. 가속화 된 네트워킹에는 가상 머신의 처리량을 향상 시킬 수 있지만 가상 머신의 할당 된 대역폭 까지만 할 수 있습니다.

Azure 가상 머신은 여기에 연결 된 하나 이상의 네트워크 인터페이스를 갖습니다. 몇 가지 해야 할 수 있습니다. 가상 머신에 할당 된 대역폭을 컴퓨터에 연결 하는 모든 네트워크 인터페이스를 통해 모든 아웃 바운드 트래픽의 합계가 표시 됩니다. 즉, 대역폭을 컴퓨터에 연결 된 네트워크 인터페이스 개수에 관계 없이 가상 컴퓨터 별로 할당 됩니다.

예상 되는 아웃 바운드 처리량 및 각 VM 크기에서 지 원하는 네트워크 인터페이스의 수에 자세히 나와 [Azure에서 가상 컴퓨터 크기에 대 한 Windows](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes?toc=%2fazure%2fvirtual-network%2ftoc.json)합니다. 최대 처리량을 확인 하려면 유형의 같은 선택한 **범용**, 한 다음 결과 페이지 (예를 들어, "Dv2 시리즈")에서 크기 계열에 대 한 섹션을 찾습니다. 각 계열에 대 한은 테이블의 마지막 열 제목은 네트워킹 사양을 제공 하는 "최대 Nic / 예상 네트워크 대역폭 (Mbps)입니다."

처리량 제한은 가상 컴퓨터에 적용됩니다. 처리량은 이러한 요인에 의해 영향을 받지 않습니다.

- **네트워크 인터페이스 수가**: 가상 머신에서 모든 아웃 바운드 트래픽의 합계는 대역폭 제한이 적용 됩니다.

- **가속화 된 네트워킹**: 이 기능을 게시 한도 달성 하는 데 유용할 수 있습니다, 있지만 변경 되지 않습니다.

- **트래픽 대상**: 아웃 바운드 제한에 대해 모든 대상 수입니다.

- **프로토콜**: 모든 프로토콜을 통한 모든 아웃 바운드 트래픽을 제한에 대해 계산 됩니다.

자세한 내용은 [가상 머신 네트워크 대역폭](http://aka.ms/AzureBandwidth)합니다.

### <a name="internet-performance-considerations"></a>인터넷 성능 고려 사항

이 문서 전체에서 설명 했 듯이 요소 인터넷과 Azure의 컨트롤 외부의 네트워크 성능이 저하 될 수 있습니다. 이러한 요인은 다음과 같습니다.

- **Latency**: 중간 네트워크 문제, "가장 짧은" 거리 경로 갖지 않는 트래픽 및 최적이 아닌 피어 링 경로 의해 두 대상 간에 왕복 시간을 달라질 수 있습니다.

- **패킷 손실**: 패킷 손실 실적이 네트워크 장치, 네트워크 정체 및 실제 경로 문제를 발생할 수 있습니다.

- **MTU 크기/조각화**: 경로 따라 조각화 패킷 배달에 영향을 줄 수 있는 데이터 도착 또는 순서 대로 도착 하는 패킷을 지연 시킬 수 있습니다.

경로 추적 된 원본 장치 및 대상 장치 간의 모든 네트워크 경로 (예: 패킷 손실 및 대기 시간) 네트워크 성능 특징을 측정 하는 데 유용한 도구 이며

### <a name="network-design-considerations"></a>네트워크 디자인 고려 사항

이 문서의 앞부분에서 설명한 고려 사항은, 함께 가상 네트워크의 토폴로지 네트워크의 성능이 저하 될 수 있습니다. 예를 들어 귀로 화물 단일 허브 가상 네트워크에 전역적으로 트래픽이 허브 및 스포크 디자인 전체 네트워크 성능이 저하 될 수 있는 네트워크 대기 시간을 소개 합니다.

네트워크 트래픽이 통과 하는 네트워크 장치 수가 전체 대기 시간에 따라서도 달라 집니다. 예를 들어 허브 및 스포크 디자인에서는 인터넷에 전송을 전에 허브 가상 어플라이언스 및 스포크 네트워크 가상 어플라이언스를 통해 트래픽을 전달 하는 경우 네트워크 가상 어플라이언스 지연이 발생할 수 있습니다.

### <a name="azure-regions-virtual-networks-and-latency"></a>Azure 지역, 가상 네트워크 및 대기 시간

지역 내에 있는 여러 데이터 센터의 azure 지역 구성 됩니다. 서로 옆에 있는 데이터이 센터 물리적으로 되지 않을 수 있습니다. 경우에 따라 10 킬로미터 만큼에서 분리 합니다. 가상 네트워크는 실제 Azure 데이터 센터 네트워크를 기반으로 논리 오버레이 합니다. 가상 네트워크에는 데이터 센터 내에서 어떤 특정 네트워크 토폴로지에 의미 하지 않습니다.

예를 들어, 동일한 가상 네트워크 및 서브넷에 있는 두 Vm 다른 랙, 행 또는 심지어 데이터 센터에 있을 수 있습니다. 광섬유 케이블 킬로미터 또는 광섬유 케이블 미터에서 분리 될 수 있습니다. 이 변형에는 다른 Vm 간의 가변 대기 시간 (몇 시간 (밀리초) 차이) 높을 수 있습니다.

Vm의 지리적 위치와 두 개의 Vm 간의 잠재적 결과 대기 시간이 가용성 집합과 가용성 영역의 구성에 의해 영향을 받을 수 있습니다. 이지만 지역의 데이터 센터 간의 거리 지역별 및 주로 영향을 받는 지역의 데이터 센터 토폴로지별 있습니다.

### <a name="source-nat-port-exhaustion"></a>NAT 포트 소모 원본

Azure에 배포 되는 공용 인터넷에서 및/또는 공용 IP 공간에 있는 Azure 외부에서 끝점과 통신할 수 있습니다. 인스턴스가 아웃 바운드 연결에서 시작 하는 경우 Azure 공용 IP 주소를 개인 IP 주소를 동적으로 매핑합니다. Azure에서이 매핑을 만든 후 반환 트래픽도 아웃 바운드에서 시작 된 흐름에 연결할 수 있습니다 개인 IP 주소 흐름이 시작 된 위치입니다.

모든 아웃 바운드 연결에 대 한 Azure Load Balancer를 일정 기간에 대 한이 매핑을 유지 관리 해야 합니다. Azure의 다중 테 넌 트 본질적으로 모든 VM에 대 한 모든 아웃 바운드 흐름에 대 한이 매핑을 유지 관리 리소스를 많이 사용 될 수 있습니다. 따라서 설정 및 Azure Virtual Network의 구성에 따라 제한이 있습니다. 또는 보다 정확 하 게 하다, Azure VM에만 가능 특정 수의 아웃 바운드 연결을 지정된 된 시간에 있습니다. 이러한 한도 도달 하면, VM는 더 많은 아웃 바운드 연결을 만들 수 없습니다.

하지만이 동작은 구성할 수 있습니다. SNAT 및 SNAT에 대 한 자세한 내용은 포트 소모를 참조 하세요 [이 문서에서는](https://docs.microsoft.com/azure/load-balancer/load-balancer-outbound-connections)합니다.

## <a name="measure-network-performance-on-azure"></a>Azure에서 네트워크 성능 측정

네트워크 대기 시간에 관련 된 다양 한이 문서의 성능 최대값 왕복 / 두 Vm 간의 시간 (RTT). 이 섹션에서는 대기 시간/RTT를 테스트 하는 방법 및 TCP 성능 및 VM 간 네트워크 성능을 테스트 하는 방법에 대 한 몇 가지 제안을 제공 합니다. 조정할 수 있습니다 하 고이 섹션에 설명 된 기술을 사용 하 여 앞에서 설명한 TCP/IP 및 네트워크 값의 성능을 테스트 합니다. 앞서 제공 된 계산에 대기 시간, MTU, MSS 및 창 크기 값을 연결 하 고 이론적인 최대값을 테스트 하는 중에 관찰 하는 실제 값을 비교할 수 있습니다.

### <a name="measure-round-trip-time-and-packet-loss"></a>측정값 왕복 시간과 패킷 손실

TCP 성능 RTT 및 패킷 손실에 크게 의존 합니다. Windows 및 Linux에서 사용할 수 있는 PING 유틸리티 RTT 및 패킷 손실을 측정 하는 가장 쉬운 방법은 제공 합니다. PING의 출력에는 원본과 대상 간의 최소/최대/평균 대기 시간이 표시 됩니다. 패킷 손실도 표시 됩니다. PING은 기본적으로 ICMP 프로토콜을 사용합니다. PsPing을 사용 하 여 TCP RTT를 테스트할 수 있습니다. 자세한 내용은 [PsPing](https://docs.microsoft.com/sysinternals/downloads/psping)합니다.

### <a name="measure-actual-throughput-of-a-tcp-connection"></a>TCP 연결의 실제 처리량 측정

NTttcp를 Linux 또는 Windows VM의 TCP 성능 테스트에 대 한 도구 이며 다양 한 TCP 설정 변경 하 고 NTttcp를 사용 하 여 혜택을 테스트할 수 있습니다. 이러한 응용 프로그램은 Azure AD Graph API를 사용할 수 있습니다. 자세한 내용은 다음 리소스를 참조하세요. 

- [대역폭/처리량 테스트 (NTttcp)](https://aka.ms/TestNetworkThroughput)

- [NTttcp 유틸리티](https://gallery.technet.microsoft.com/NTttcp-Version-528-Now-f8b12769)

### <a name="measure-actual-bandwidth-of-a-virtual-machine"></a>가상 컴퓨터의 실제 대역폭 측정값

다양 한 형식의 VM 가속화 된 네트워킹, 성능을 테스트 하 고 iPerf 라는 도구를 사용 하 여 등에 수 있습니다. iPerf는 Linux 및 Windows에서 사용할 수도 있습니다. iPerf는 TCP 또는 UDP를 사용 하 여 전반적인 네트워크 처리량 테스트를 수 있습니다. iPerf TCP 처리량 테스트 됩니다 (예: 대기 시간 및 RTT)이이 문서에서 설명한 요인에 의해 영향을 받습니다. 따라서 UDP 최대 처리량을 테스트 하려는 경우 더 나은 결과 생성할 수 있습니다.

자세한 내용은 다음 문서를 참조하세요.

- [Expressroute 네트워크 성능 문제 해결](https://docs.microsoft.com/azure/expressroute/expressroute-troubleshooting-network-performance)

- [가상 네트워크에 대한 VPN 처리량의 유효성을 검사하는 방법](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-validate-throughput-to-vnet)

### <a name="detect-inefficient-tcp-behaviors"></a>비효율적인 TCP 동작 감지

패킷 캡처에 Azure 고객은 네트워크 성능 문제를 나타낼 수 있는 TCP 플래그 (SACK, 중복 된 ACK, 재전송 및 고속 재전송)를 사용 하 여 TCP 패킷 나타날 수 있습니다. 특히 이러한 패킷은 패킷 손실이 발생 하는 네트워크 비효율성을 나타냅니다. 하지만 패킷 손실 되지 않습니다 반드시 Azure 성능 문제로 인해 발생 하기도 합니다. 성능 문제 때문일 수의 응용 프로그램 문제, 운영 체제 문제 또는 다른 문제는 Azure 플랫폼에 직접 연결 되지 않을 수 있습니다.

또한 일부 재전송 및 중복 Ack은 네트워크에서 일반 유지 합니다. TCP 프로토콜 안정적으로 빌드됩니다. 패킷 캡처에 이러한 TCP 패킷의 증거는 과도 한 경우가 시스템 네트워크 문제를 나타내지는지 않습니다.

그러나 이러한 패킷 종류는 TCP 처리량이 문서의 다른 섹션에 설명 된 이유로 최대 성능을 달성 하지는 있음을 나타냅니다.

## <a name="next-steps"></a>다음 단계

에 대 한 기타 고려 사항에 대 한 읽기 하려는 Azure Vm에 대 한 TCP/IP 성능 조정에 대 한 알아보았습니다 했으므로 [가상 네트워크 계획 수립](https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-vnet-plan-design-arm) 또는 [연결한 가상 네트워크를 구성 하는 방법에 대 한 자세한 정보 ](https://docs.microsoft.com/en-us/azure/virtual-network/).
